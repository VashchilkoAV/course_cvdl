## Задание 2. Реализация нейронной сети для детекции объектов CenterNet.

### Описание
В этом задании вам необходимо реализовать на torch упрощенную версию сети CenterNet, статья [Objects as Points](https://arxiv.org/pdf/1904.07850.pdf).
Аналогично первому заданию, имеется пакет-заготовка для реализации сети. Ваша задача - разобраться в статье и реализовать те фрагменты, где в коде встречается
 `raise NotImplementedError` (а также `pass #NotImplemented`).

Замечания:
- изучение кода [оригинальной](https://github.com/xingyizhou/CenterNet) CenterNet приветствуется.
- не перепутайте с другой статьей [CenterNet: Keypoint Triplets for Object Detection](https://arxiv.org/abs/1904.08189): она сложнее!

### Оценка
Оценка ставится инструкторами вручную, с проверкой реализации компонентов и результатов тренировки.
В срок [10.10 – 31.10], `максимум баллов за задание` = 20. Начиная с 31.10, `максимум баллов за задание` = 10. Начиная с 15.12, баллы не ставятся.

### Список компонентов, которые надо реализовать
- abbyy_course_cvdl_t2.backbone.UpscaleTwiceLayer (2 балла)
- abbyy_course_cvdl_t2.backbone.HeadlessResnet34 (2 балла)
- abbyy_course_cvdl_t2.head.CenterNetHead (2 балла)
- abbyy_course_cvdl_t2.loss.CenterNetLoss (2 балла)
- abbyy_course_cvdl_t2.convert.ObjectsToPoints (3 балла)
- abbyy_course_cvdl_t2.convert.PointsToObjects (4 балла)
- abbyy_course_cvdl_t2.network.PointsNonMaxSuppression (3 балла)

Корректность компонентов оценивается по тестам и "на глаз": если компонент не проходит почему-то тест, но по смыслу верен, он будет засчитан
Все компоненты - 18 баллов. Еще 2 балла даются за "успешную" тренировку на TinyCoco( см. ниже ).

### Баллы за тренировку на TinyCoco
Для удобства отладки приложен `task2/checks_notebook.ipynb`, в котором сначала вызываются компоненты по-отдельности, а затем выполняется тренировка на датасете TinyCoco (игрушечная)
Датасет TinyCoco приложен в task2/data, для тренировки на нем достаточно CPU.

Тренировку можно делать с pretrained бэкбоном.

Тренировка считается успешной, если лосс в логах тренировки (выводятся в checks_notebook.ipynb) стабильно понижается.

### Дополнительные баллы за тренировку на CocoText
При тренировке на TinyCoco (~10 изображений) чего-то даже отдаленно работающего не получится в любом случае - слишком мало данных. Чтобы проверить, работоспособен ли написанный вами детектор, его надо обучить на полноразмерном датасете (~10K изображений).

В ноутбуке `train_cocotext.ipynb` приведена тренировка детектора на [CocoText](https://bgshih.github.io/cocotext/) - датасете с изображениями из COCO, на которых размечен весь текст.

Вам необходимо скачать изображения [COCO](https://cocodataset.org) версии 2014 года и разметку cocotext.v2.json с сайта CocoText, прогнать обучение в ноутбуке на своей реализации CenterNet и удостовериться, что ваше детектор после обучения выдаёт что-то похожее на детекции текста.

За успешную тренировку на CocoText дополнительно начисляется 3 балла. У этого пункта дедлайн - конец семестра, т.е. получить доп. баллы за успешную тренировку работающего детектора можно и после 31.10.

## Результаты проверки

1. backbone.py::UpscaleTwiceLayer
- Замечание  - используется единственный ConvTranspose. Технически, слой выполняет задачу (и проходит тесты), но на практике Transpose свёртки редко ставятся совсем в одиночку. В simple baselines (на который ссылается CenterNet, ссылка [55]) используется (ConvTranspose + BN + ReLU). Либо после ConvTranspose ставится блок Conv+BN+ReLU. Слои UpscaleTwice в backbone стоят подряд, и использование N чистых ConvTranspose подряд скорее всего приведет к checkboard pattern (разбирали примеры на одном из занятий).

2. head.py::CenterNetHead
- Ошибка - в probs_head не учтён background класс. В probs_head используется `c_classes` каналов с последующим Softmax - значит, что один из c_classes будет обязательно иметь высокую вероятность в каждой локации. Но в подготовке данных (в ObjectsToPoints) в большинстве локаций везде 0 (потому что там нет объектов). Получается, сеть обязана во всех локациях предсказать хоть в одном канале ~1, но в таргете в большинстве локаций везде 0 - лосс будет всегда высоким (и логически неверным). Решается использованием в probs_head (c_classes+1) каналов с отбрасыванием 0 или последнего канала (будет соответствовать background). Также можно решить это, использовав сигмоиду вместо softmax - тогда сеть сможет предсказывать везде 0. Также можно решить, явно учтя в ObjectsToPoints, PointsToObjects и CenterNetLoss одного канала на background (но это более сложный путь). 
- Замечание - в offset_head нет никакой активации. В принципе, как-то это будет работать и в таком варианте, но учитывая что отступы могут быть только от 0 до 1 - логично было бы использовать сигмоиду

3. convert.py::ObjectsToPoints
- Замечание - в compute_objects_locations вместо вложенных циклов можно использовать torch.repeat_interleave или !!!!!!!!!!!!

4. convert.py::PointsToObjects
- Замечание - в objects[b, k, 5] должно быть не `(scores[b, k] > self.min_conf)`, т.е. бинарная величина, а `score[b, k] * (scores[b, k] > self.min_conf)` т.е. confidence, если он выше порога - иначе 0

5. Тренировка на TinyCoco
Замечание - Интересно, что несмотря на ошибку в head.py, лосс в тренировке падает. В ноутбуке check_notebook c c_classes=1. В голове после применения Softmax выходу с числом каналов 1 результат будет всегда одинаковый - вероятность равная 1 во всех локациях, а при вероятности 1 в лоссе должны возникнуть nan в torch.log(1-x). Тот случай, когда код работает, и не очень понятно - почему.

Итого:
- "замечания" не влияют на баллы
- ошибка в head: -1 балл

19 баллов
