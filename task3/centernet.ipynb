{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from torchvision.datasets import CocoDetection\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abbyy_course_cvdl_t3.coco_text import COCO_Text\n",
    "from abbyy_course_cvdl_t3 import coco_evaluation\n",
    "from abbyy_course_cvdl_t3.utils import evaluate_ap_from_cocotext_json\n",
    "from abbyy_course_cvdl_t3.utils import dump_detections_to_cocotext_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abbyy_course_cvdl_t2\n",
    "from abbyy_course_cvdl_t2.convert import PointsToObjects, ObjectsToPoints\n",
    "from abbyy_course_cvdl_t2.impl.train import train\n",
    "from abbyy_course_cvdl_t2.impl.data import CocoTextDetection, CocoDetectionPrepareTransform\n",
    "from abbyy_course_cvdl_t2.network import CenterNet\n",
    "from abbyy_course_cvdl_t2.loss import CenterNetLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../task3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = Path(\"/home/alexander/Downloads/coco2014/\")\n",
    "anno_path = base / 'cocotext.v2.json'\n",
    "images_path = base / 'images/train2014'\n",
    "\n",
    "assert anno_path.exists(), \"Set your own path to annotation\"\n",
    "assert images_path.exists(), \"Set your own path to images\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = COCO_Text(anno_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(image_np, yc_t, xc_t, hy_t, wx_t, color=(0, 255, 0), thick=1):\n",
    "    img = np.array(image_np)\n",
    "    num_boxes = len(yc_t)\n",
    "    val = np.array(color)\n",
    "    for idx in range(num_boxes):\n",
    "        yc = yc_t[idx]\n",
    "        xc = xc_t[idx]\n",
    "        hy = hy_t[idx]\n",
    "        wx = wx_t[idx]\n",
    "        img[\n",
    "                yc - hy//2 - thick : yc - hy//2 + thick, \n",
    "                xc - wx//2 : xc + wx//2, \n",
    "        ] = val\n",
    "        img[\n",
    "                yc + hy//2 - thick : yc + hy//2 + thick, \n",
    "                xc - wx//2 : xc + wx//2, \n",
    "        ] = val\n",
    "\n",
    "        img[\n",
    "                yc - hy//2 : yc + hy//2, \n",
    "                (xc - wx//2 - thick): (xc - wx//2 + thick), \n",
    "        ] = val\n",
    "        img[\n",
    "                yc - hy//2 : yc + hy//2, \n",
    "                xc + wx//2 - thick: xc + wx//2 + thick, \n",
    "        ] = val\n",
    "    return img\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 2\n",
    "input_shape = (256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = CocoTextDetection(\n",
    "    Path(\"/home/alexander/Downloads/coco2014/images/train2014\"),\n",
    "    Path(\"/home/alexander/Downloads/coco2014/cocotext.v2.json\"),\n",
    "    transforms=CocoDetectionPrepareTransform(size=input_shape, #ids_map = {1: 0, 0: 0}\n",
    "),\n",
    "    area_fraction_threshold=1/32/32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_val = CocoTextDetection(\n",
    "    images_path,\n",
    "    Path(anno_path),\n",
    "    transforms=CocoDetectionPrepareTransform(size=input_shape, #ids_map = {1: 0, 0: 0}\n",
    "    ),\n",
    "    area_fraction_threshold=1/32/32,\n",
    "    split='val'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = ds_train[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(\n",
    "    ds_train, batch_size=2, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valloader = torch.utils.data.DataLoader(\n",
    "    ds_val, batch_size=1, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = CenterNet(head_kwargs={'c_classes': n_classes}, nms_kwargs={'kernel_size': 5})\n",
    "crit = CenterNetLoss(obj_to_points=ObjectsToPoints(num_classes=n_classes))#, l_size_lambda=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = train(ds_train, net=net, criterion=crit, batch_size=64, epochs=2, device=torch.device(device), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, 'centernet.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net = torch.load('centernet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.to(device)\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = ds_train[6]\n",
    "\n",
    "with torch.no_grad():\n",
    "    yp_heat = net(x[None].to('cuda:0')).cpu()    \n",
    "    yp = net(x[None].to('cuda:0'), return_objects=True).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('class 0 heatmap')\n",
    "plt.matshow(yp_heat[0][0])\n",
    "plt.show()\n",
    "\n",
    "print('class 1 heatmap')\n",
    "plt.matshow(yp_heat[0][1])\n",
    "plt.show()\n",
    "\n",
    "print('w_heatmap')\n",
    "plt.matshow(yp_heat[0][-1])\n",
    "plt.show()\n",
    "\n",
    "print('h heatmap')\n",
    "plt.matshow(yp_heat[0][-2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(image_np, yc_t, xc_t, hy_t, wx_t, color=(0, 255, 0), thick=1):\n",
    "    img = np.array(image_np)\n",
    "    num_boxes = len(yc_t)\n",
    "    val = np.array(color)\n",
    "    for idx in range(num_boxes):\n",
    "        yc = yc_t[idx]\n",
    "        xc = xc_t[idx]\n",
    "        hy = hy_t[idx]\n",
    "        wx = wx_t[idx]\n",
    "\n",
    "        hyd2 = torch.div(hy, 2, rounding_mode='trunc')\n",
    "        wxd2 = torch.div(wx, 2, rounding_mode='trunc')\n",
    "\n",
    "        img[\n",
    "                yc - hyd2 - thick : yc - hyd2 + thick, \n",
    "                xc - wxd2 : xc + wxd2, \n",
    "        ] = val\n",
    "        img[\n",
    "                yc + hyd2 - thick : yc + hyd2 + thick, \n",
    "                xc - wxd2 : xc + wxd2, \n",
    "        ] = val\n",
    "\n",
    "        img[\n",
    "                yc - hyd2 : yc + hyd2, \n",
    "                (xc - wxd2 - thick): (xc - wxd2 + thick), \n",
    "        ] = val\n",
    "        img[\n",
    "                yc - hyd2 : yc + hyd2, \n",
    "                xc + wxd2 - thick: xc + wxd2 + thick, \n",
    "        ] = val\n",
    "    return img\n",
    "\n",
    "\n",
    "rnd = lambda x: torch.round(x).long()\n",
    "\n",
    "plt.imshow(draw_boxes(\n",
    "    x.permute(1, 2, 0).numpy(), rnd(yp[0, :3, 0]), rnd(yp[0, :3, 1]), rnd(yp[0, :3, 2]), rnd(yp[0, :3, 3]) \n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(draw_boxes(\n",
    "    x.permute(1, 2, 0).numpy(), rnd(y[:, 0]), rnd(y[:, 1]), rnd(y[:, 2]), rnd(y[:, 3]) \n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net = torch.load('centernet.pth')\n",
    "\n",
    "def postprocess(data, target_shape, input_shape, min_conf=0.1):\n",
    "    unf = {}\n",
    "    res = data[data[:, -1] > min_conf]\n",
    "    boxes = res[:, :4].copy()\n",
    "    b = boxes.copy()\n",
    "    boxes[:, 0] = (boxes[:, 0] - b[:, 2] / 2) * (target_shape[0] / input_shape[0])\n",
    "    boxes[:, 1] = (boxes[:, 1] - b[:, 3] / 2) * (target_shape[1] / input_shape[1])\n",
    "    boxes[:, 2] = boxes[:, 2] * (target_shape[0] / input_shape[0])\n",
    "    boxes[:, 3] = boxes[:, 3] * (target_shape[1] / input_shape[1])\n",
    "    unf['boxes'] = boxes\n",
    "    unf['scores'] = res[:, -2]\n",
    "    return unf\n",
    "\n",
    "\n",
    "def extract_results(min_conf=0.1):\n",
    "    prepared_preds = []\n",
    "    img_ids = []\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    for num, img_id in enumerate(tqdm(ds_val.ids)):\n",
    "        img_id = int(img_id)\n",
    "        img_meta = ct.loadImgs(ids=[img_id])[0]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = ds_val[num][0]\n",
    "            \n",
    "            pred = net(\n",
    "                x[None, ...].to(device),\n",
    "                return_objects=True\n",
    "            )[0].detach().cpu().numpy()\n",
    "            prepared_preds.append(\n",
    "                postprocess(pred, target_shape=(img_meta['height'], img_meta['width']), input_shape=input_shape, min_conf=min_conf)\n",
    "            )\n",
    "            img_ids.append(img_id)\n",
    "\n",
    "    scores = np.concatenate([u['scores'] for u in prepared_preds], axis=0)\n",
    "    boxes = np.concatenate([u['boxes'] for u in prepared_preds], axis=0)\n",
    "\n",
    "    image_ids = []\n",
    "    for num, i in enumerate(img_ids):\n",
    "        image_ids += [i] * len(prepared_preds[num]['boxes'])\n",
    "    image_ids = np.array(image_ids)\n",
    "\n",
    "    #print(len(image_ids.tolist()),\n",
    "    #    len(boxes[:, 0].tolist()),\n",
    "    #    len(boxes[:, 1].tolist()),\n",
    "    #    len(boxes[:, 2].tolist()),\n",
    "    #    len(boxes[:, 3].tolist()),\n",
    "    #    len(scores.tolist()))\n",
    "\n",
    "    dump_detections_to_cocotext_json(\n",
    "        image_ids = image_ids.tolist(),\n",
    "        xlefts=boxes[:, 0].tolist(),\n",
    "        ytops=boxes[:, 1].tolist(),\n",
    "        widths=boxes[:, 2].tolist(),\n",
    "        heights=boxes[:, 3].tolist(),\n",
    "        scores=scores.tolist(),\n",
    "        path=f'predictions_{min_conf}.json'\n",
    "    )\n",
    "    \n",
    "    ap, prec, rec = evaluate_ap_from_cocotext_json(\n",
    "    coco_text=ct,\n",
    "    path=f'predictions_{min_conf}.json'\n",
    "    )\n",
    "    return (ap, prec, rec, min_conf, len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_conf=0.1\n",
    "ap, prec, rec, conf, n_obj = extract_results(min_conf=min_conf)\n",
    "\n",
    "print(f\"Итоговый скор AP на val для min_conf={min_conf}: {ap}\")\n",
    "\n",
    "plt.plot(prec, rec)\n",
    "plt.xlabel('precision')\n",
    "plt.ylabel('recall')\n",
    "plt.title('PR curve')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best AP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aps = []\n",
    "\n",
    "for min_conf in np.linspace(0.1, 0.3, 5):\n",
    "    ap, prec, rec, conf, n_obj = extract_results(min_conf=min_conf)\n",
    "\n",
    "    aps.append((ap, min_conf))\n",
    "\n",
    "    print(n_obj)\n",
    "    print(f\"Итоговый скор AP на val для min_conf={min_conf}: {ap}\")\n",
    "    plt.plot(prec, rec)\n",
    "    plt.xlabel('precision')\n",
    "    plt.ylabel('recall')\n",
    "    plt.title('PR curve')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_values = sorted(aps, key=lambda x: x[0])[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4f782c782e2a32bb2802e651bce4745c161372ca597e1c0a908f1fe5f8326707"
  },
  "kernelspec": {
   "display_name": "Python 3.10.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
