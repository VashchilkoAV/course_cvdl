{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from torchvision.datasets import CocoDetection\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abbyy_course_cvdl_t3.coco_text import COCO_Text\n",
    "from abbyy_course_cvdl_t3 import coco_evaluation\n",
    "from abbyy_course_cvdl_t3.utils import evaluate_ap_from_cocotext_json\n",
    "from abbyy_course_cvdl_t3.utils import dump_detections_to_cocotext_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alexander/computerScience/phystech/9sem/abbyy/course_cvdl/task2\n"
     ]
    }
   ],
   "source": [
    "cd ../task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abbyy_course_cvdl_t2\n",
    "from abbyy_course_cvdl_t2.convert import PointsToObjects, ObjectsToPoints\n",
    "from abbyy_course_cvdl_t2.impl.train import train\n",
    "from abbyy_course_cvdl_t2.impl.data import CocoTextDetection, CocoDetectionPrepareTransform\n",
    "from abbyy_course_cvdl_t2.network import CenterNet\n",
    "from abbyy_course_cvdl_t2.loss import CenterNetLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alexander/computerScience/phystech/9sem/abbyy/course_cvdl/task3\n"
     ]
    }
   ],
   "source": [
    "cd ../task3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = Path(\"/home/alexander/Downloads/coco2014/\")\n",
    "anno_path = base / 'cocotext.v2.json'\n",
    "images_path = base / 'images/train2014'\n",
    "\n",
    "assert anno_path.exists(), \"Set your own path to annotation\"\n",
    "assert images_path.exists(), \"Set your own path to images\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:02.482653\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "ct = COCO_Text(anno_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(image_np, yc_t, xc_t, hy_t, wx_t, color=(0, 255, 0), thick=1):\n",
    "    img = np.array(image_np)\n",
    "    num_boxes = len(yc_t)\n",
    "    val = np.array(color)\n",
    "    for idx in range(num_boxes):\n",
    "        yc = yc_t[idx]\n",
    "        xc = xc_t[idx]\n",
    "        hy = hy_t[idx]\n",
    "        wx = wx_t[idx]\n",
    "        img[\n",
    "                yc - hy//2 - thick : yc - hy//2 + thick, \n",
    "                xc - wx//2 : xc + wx//2, \n",
    "        ] = val\n",
    "        img[\n",
    "                yc + hy//2 - thick : yc + hy//2 + thick, \n",
    "                xc - wx//2 : xc + wx//2, \n",
    "        ] = val\n",
    "\n",
    "        img[\n",
    "                yc - hy//2 : yc + hy//2, \n",
    "                (xc - wx//2 - thick): (xc - wx//2 + thick), \n",
    "        ] = val\n",
    "        img[\n",
    "                yc - hy//2 : yc + hy//2, \n",
    "                xc + wx//2 - thick: xc + wx//2 + thick, \n",
    "        ] = val\n",
    "    return img\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = CocoTextDetection(\n",
    "    Path(\"/home/alexander/Downloads/coco2014/images/train2014\"),\n",
    "    Path(\"/home/alexander/Downloads/coco2014/cocotext.v2.json\"),\n",
    "    transforms=CocoDetectionPrepareTransform(size=(256,256), #ids_map = {1: 0, 0: 0}\n",
    "),\n",
    "    area_fraction_threshold=1/32/32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_val = CocoTextDetection(\n",
    "    images_path,\n",
    "    Path(anno_path),\n",
    "    transforms=CocoDetectionPrepareTransform(size=(256,256), #ids_map = {1: 0, 0: 0}\n",
    "    ),\n",
    "    area_fraction_threshold=1/32/32,\n",
    "    split='val'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = ds_train[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[133.8933,  60.5600,  12.5333,  13.4400,   1.0000,   1.0000],\n",
       "        [131.5200, 156.7800,  11.5200,  19.8800,   1.0000,   1.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_58128/4024544524.py:11: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  yc - hy//2 - thick : yc - hy//2 + thick,\n",
      "/tmp/ipykernel_58128/4024544524.py:12: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  xc - wx//2 : xc + wx//2,\n",
      "/tmp/ipykernel_58128/4024544524.py:15: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  yc + hy//2 - thick : yc + hy//2 + thick,\n",
      "/tmp/ipykernel_58128/4024544524.py:16: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  xc - wx//2 : xc + wx//2,\n",
      "/tmp/ipykernel_58128/4024544524.py:20: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  yc - hy//2 : yc + hy//2,\n",
      "/tmp/ipykernel_58128/4024544524.py:21: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  (xc - wx//2 - thick): (xc - wx//2 + thick),\n",
      "/tmp/ipykernel_58128/4024544524.py:24: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  yc - hy//2 : yc + hy//2,\n",
      "/tmp/ipykernel_58128/4024544524.py:25: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  xc + wx//2 - thick: xc + wx//2 + thick,\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2ab3faacd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rnd = lambda x: torch.round(x).long()\n",
    "\n",
    "plt.imshow(draw_boxes(\n",
    "    x.permute(1, 2, 0).numpy(), rnd(y[:, 0]), rnd(y[:, 1]), rnd(y[:, 2]), rnd(y[:, 3]) \n",
    "))\n",
    "#plt.imshow(x.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(\n",
    "    ds_train, batch_size=2, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "valloader = torch.utils.data.DataLoader(\n",
    "    ds_val, batch_size=1, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = CenterNet(head_kwargs={'c_classes': n_classes}, nms_kwargs={'kernel_size': 5})\n",
    "crit = CenterNetLoss(obj_to_points=ObjectsToPoints(num_classes=n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0|0; total loss:1257.8092041015625\n",
      "last losses: [37564.76953125, 54.00093460083008, 115.50421142578125]\n",
      "epoch 0|30; total loss:1886.2388916015625\n",
      "last losses: [322.9469909667969, 43.29220962524414, 46.006858825683594]\n",
      "epoch 0|60; total loss:327.7005310058594\n",
      "last losses: [213.59347534179688, 42.46577453613281, 35.97744369506836]\n",
      "epoch 0|90; total loss:288.3876647949219\n",
      "last losses: [177.9588623046875, 36.91019821166992, 42.914451599121094]\n",
      "epoch 0|120; total loss:259.88409423828125\n",
      "last losses: [148.3211669921875, 35.435909271240234, 29.899492263793945]\n",
      "epoch 0|150; total loss:260.4101257324219\n",
      "last losses: [199.35916137695312, 47.88767623901367, 47.61766815185547]\n",
      "epoch 0|180; total loss:252.7855224609375\n",
      "last losses: [168.62100219726562, 39.29122543334961, 45.07689666748047]\n",
      "epoch 0|210; total loss:247.0817413330078\n",
      "last losses: [132.96917724609375, 33.38351821899414, 47.74021530151367]\n",
      "epoch 0|240; total loss:237.0569610595703\n",
      "last losses: [150.7279052734375, 42.22948455810547, 48.17622375488281]\n",
      "epoch 0|270; total loss:245.4009246826172\n",
      "last losses: [195.0806884765625, 47.680023193359375, 54.75390625]\n",
      "epoch 1|0; total loss:7.7048187255859375\n",
      "last losses: [148.94662475585938, 40.77326583862305, 41.424652099609375]\n",
      "epoch 1|30; total loss:228.0224151611328\n",
      "last losses: [199.58555603027344, 56.279441833496094, 56.05309295654297]\n",
      "epoch 1|60; total loss:233.03048706054688\n",
      "last losses: [113.02117919921875, 28.38452911376953, 24.5447998046875]\n",
      "epoch 1|90; total loss:226.1541290283203\n",
      "last losses: [181.17506408691406, 50.528114318847656, 51.19282150268555]\n",
      "epoch 1|120; total loss:223.28509521484375\n",
      "last losses: [126.02275848388672, 39.639060974121094, 38.687538146972656]\n",
      "epoch 1|150; total loss:224.87437438964844\n",
      "last losses: [138.0122528076172, 38.266292572021484, 44.383949279785156]\n",
      "epoch 1|180; total loss:230.99989318847656\n",
      "last losses: [113.41924285888672, 35.55143356323242, 38.117164611816406]\n",
      "epoch 1|210; total loss:220.8636474609375\n",
      "last losses: [142.25091552734375, 37.962982177734375, 45.333953857421875]\n",
      "epoch 1|240; total loss:217.14508056640625\n",
      "last losses: [131.84768676757812, 38.460330963134766, 37.54597091674805]\n",
      "epoch 1|270; total loss:217.3092041015625\n",
      "last losses: [159.09365844726562, 46.657005310058594, 36.92225646972656]\n",
      "epoch 2|0; total loss:6.89418888092041\n",
      "last losses: [126.57804870605469, 42.46294403076172, 37.784645080566406]\n",
      "epoch 2|30; total loss:205.22605895996094\n",
      "last losses: [121.88565826416016, 38.931095123291016, 35.21778869628906]\n",
      "epoch 2|60; total loss:204.20240783691406\n",
      "last losses: [126.3035659790039, 45.53541564941406, 41.90829086303711]\n",
      "epoch 2|90; total loss:205.67950439453125\n",
      "last losses: [131.40150451660156, 41.081886291503906, 47.287235260009766]\n",
      "epoch 2|120; total loss:210.57998657226562\n",
      "last losses: [125.32350158691406, 35.78570556640625, 36.55516815185547]\n",
      "epoch 2|150; total loss:211.20631408691406\n",
      "last losses: [153.99777221679688, 43.378841400146484, 41.62492370605469]\n",
      "epoch 2|180; total loss:212.65264892578125\n",
      "last losses: [167.87754821777344, 45.994937896728516, 52.52390670776367]\n",
      "epoch 2|210; total loss:208.4068603515625\n",
      "last losses: [136.09967041015625, 39.18059158325195, 38.572715759277344]\n",
      "epoch 2|240; total loss:199.95706176757812\n",
      "last losses: [115.05233764648438, 34.97947311401367, 27.804967880249023]\n",
      "epoch 2|270; total loss:202.82821655273438\n",
      "last losses: [132.51707458496094, 38.86554718017578, 27.759429931640625]\n",
      "epoch 3|0; total loss:6.0923662185668945\n",
      "last losses: [113.88127899169922, 38.070926666259766, 30.818782806396484]\n",
      "epoch 3|30; total loss:188.5523681640625\n",
      "last losses: [101.93946838378906, 31.831344604492188, 31.749670028686523]\n",
      "epoch 3|60; total loss:181.52865600585938\n",
      "last losses: [108.63414001464844, 35.30819320678711, 29.022380828857422]\n",
      "epoch 3|90; total loss:185.03427124023438\n",
      "last losses: [114.71406555175781, 36.27012634277344, 33.63750076293945]\n",
      "epoch 3|120; total loss:183.1610870361328\n",
      "last losses: [122.75579833984375, 39.91231918334961, 36.07256317138672]\n",
      "epoch 3|150; total loss:198.91629028320312\n",
      "last losses: [113.89671325683594, 39.78018569946289, 33.93708038330078]\n",
      "epoch 3|180; total loss:198.65042114257812\n",
      "last losses: [101.50048828125, 32.12173843383789, 25.735660552978516]\n",
      "epoch 3|210; total loss:177.00450134277344\n",
      "last losses: [92.64434814453125, 31.43411636352539, 22.975648880004883]\n",
      "epoch 3|240; total loss:192.4524383544922\n",
      "last losses: [125.60804748535156, 41.366817474365234, 32.19979476928711]\n",
      "epoch 3|270; total loss:188.68692016601562\n",
      "last losses: [110.88671112060547, 36.653968811035156, 36.55520248413086]\n",
      "epoch 4|0; total loss:6.424973011016846\n",
      "last losses: [117.94281005859375, 40.907657623291016, 33.89871597290039]\n",
      "epoch 4|30; total loss:174.76422119140625\n",
      "last losses: [94.09423828125, 37.81956481933594, 30.480594635009766]\n",
      "epoch 4|60; total loss:168.01699829101562\n",
      "last losses: [85.25274658203125, 31.848865509033203, 28.137657165527344]\n",
      "epoch 4|90; total loss:168.45907592773438\n",
      "last losses: [95.4010009765625, 36.9999885559082, 23.549415588378906]\n",
      "epoch 4|120; total loss:169.25416564941406\n",
      "last losses: [98.84100341796875, 36.61601257324219, 28.389522552490234]\n",
      "epoch 4|150; total loss:172.17770385742188\n",
      "last losses: [120.36763763427734, 40.475608825683594, 33.876014709472656]\n",
      "epoch 4|180; total loss:168.3966827392578\n",
      "last losses: [89.33155059814453, 30.260101318359375, 28.42371368408203]\n",
      "epoch 4|210; total loss:170.94265747070312\n",
      "last losses: [112.87482452392578, 41.47867965698242, 36.40800094604492]\n",
      "epoch 4|240; total loss:180.7386932373047\n",
      "last losses: [91.518310546875, 39.067649841308594, 33.15834426879883]\n",
      "epoch 4|270; total loss:176.4507293701172\n",
      "last losses: [115.32783508300781, 42.90989303588867, 46.494163513183594]\n",
      "epoch 5|0; total loss:5.16301155090332\n",
      "last losses: [88.43972778320312, 38.33673095703125, 28.113872528076172]\n",
      "epoch 5|30; total loss:151.4257049560547\n",
      "last losses: [77.22813415527344, 34.04052734375, 26.080265045166016]\n",
      "epoch 5|60; total loss:152.30709838867188\n",
      "last losses: [92.4698486328125, 40.44424057006836, 31.328250885009766]\n",
      "epoch 5|90; total loss:157.61155700683594\n",
      "last losses: [118.67825317382812, 51.28284454345703, 41.51173782348633]\n",
      "epoch 5|120; total loss:155.64390563964844\n",
      "last losses: [75.09115600585938, 29.612018585205078, 24.49948501586914]\n",
      "epoch 5|150; total loss:161.13601684570312\n",
      "last losses: [93.70329284667969, 33.77014923095703, 32.24445343017578]\n",
      "epoch 5|180; total loss:154.9827117919922\n",
      "last losses: [93.15782165527344, 34.156402587890625, 30.444355010986328]\n",
      "epoch 5|210; total loss:164.3546600341797\n",
      "last losses: [82.34799194335938, 33.60176086425781, 26.454349517822266]\n",
      "epoch 5|240; total loss:158.56719970703125\n",
      "last losses: [90.47012329101562, 36.66926193237305, 22.755687713623047]\n",
      "epoch 5|270; total loss:162.88124084472656\n",
      "last losses: [87.59796142578125, 31.59857940673828, 22.549758911132812]\n",
      "epoch 6|0; total loss:3.650953769683838\n",
      "last losses: [61.888648986816406, 27.581802368164062, 20.05816078186035]\n",
      "epoch 6|30; total loss:140.37673950195312\n",
      "last losses: [77.77310180664062, 35.80591583251953, 29.670886993408203]\n",
      "epoch 6|60; total loss:136.3883819580078\n",
      "last losses: [57.371238708496094, 27.085020065307617, 19.548816680908203]\n",
      "epoch 6|90; total loss:140.22128295898438\n",
      "last losses: [68.74703979492188, 36.32894515991211, 25.638341903686523]\n",
      "epoch 6|120; total loss:143.39700317382812\n",
      "last losses: [78.27120208740234, 39.29789733886719, 25.247989654541016]\n",
      "epoch 6|150; total loss:137.73643493652344\n",
      "last losses: [91.2200927734375, 41.23171615600586, 32.59044647216797]\n",
      "epoch 6|180; total loss:137.86448669433594\n",
      "last losses: [78.21719360351562, 31.385831832885742, 25.421527862548828]\n",
      "epoch 6|210; total loss:142.06344604492188\n",
      "last losses: [89.29499816894531, 34.71697998046875, 30.043867111206055]\n",
      "epoch 6|240; total loss:154.5917205810547\n",
      "last losses: [88.3561782836914, 36.84242248535156, 24.039283752441406]\n",
      "epoch 6|270; total loss:146.5916290283203\n",
      "last losses: [101.71893310546875, 45.308258056640625, 32.27381134033203]\n",
      "epoch 7|0; total loss:4.066827774047852\n",
      "last losses: [66.73768615722656, 33.453216552734375, 21.813928604125977]\n",
      "epoch 7|30; total loss:125.17265319824219\n",
      "last losses: [62.946937561035156, 36.13518524169922, 26.331266403198242]\n",
      "epoch 7|60; total loss:122.77273559570312\n",
      "last losses: [55.9045524597168, 33.33431625366211, 24.33371353149414]\n",
      "epoch 7|90; total loss:124.54411315917969\n",
      "last losses: [68.19429016113281, 37.02985382080078, 29.651729583740234]\n",
      "epoch 7|120; total loss:123.2629623413086\n",
      "last losses: [76.41842651367188, 40.333534240722656, 32.329803466796875]\n",
      "epoch 7|150; total loss:122.84725952148438\n",
      "last losses: [60.37144088745117, 30.930612564086914, 21.17904281616211]\n",
      "epoch 7|180; total loss:128.7171630859375\n",
      "last losses: [75.47285461425781, 34.9053840637207, 24.899398803710938]\n",
      "epoch 7|210; total loss:126.420166015625\n",
      "last losses: [75.70987701416016, 36.76362228393555, 24.52528953552246]\n",
      "epoch 7|240; total loss:135.36648559570312\n",
      "last losses: [71.1955795288086, 31.372528076171875, 24.624271392822266]\n",
      "epoch 7|270; total loss:130.72381591796875\n",
      "last losses: [67.37344360351562, 38.63618087768555, 28.38628387451172]\n",
      "epoch 8|0; total loss:3.277519941329956\n",
      "last losses: [48.07787322998047, 29.216876983642578, 21.03084945678711]\n",
      "epoch 8|30; total loss:110.69842529296875\n",
      "last losses: [49.14519119262695, 30.508007049560547, 15.332771301269531]\n",
      "epoch 8|60; total loss:106.93624114990234\n",
      "last losses: [52.04475402832031, 35.424896240234375, 22.3337345123291]\n",
      "epoch 8|90; total loss:104.72913360595703\n",
      "last losses: [55.047874450683594, 31.38394546508789, 20.58317756652832]\n",
      "epoch 8|120; total loss:113.1119155883789\n",
      "last losses: [58.08849334716797, 36.1700325012207, 24.73906707763672]\n",
      "epoch 8|150; total loss:114.37229919433594\n",
      "last losses: [47.57179641723633, 29.493375778198242, 20.482776641845703]\n",
      "epoch 8|180; total loss:114.78321075439453\n",
      "last losses: [62.842506408691406, 37.64180374145508, 23.57221221923828]\n",
      "epoch 8|210; total loss:118.49417114257812\n",
      "last losses: [51.52046203613281, 28.094619750976562, 21.739925384521484]\n",
      "epoch 8|240; total loss:116.14826202392578\n",
      "last losses: [59.27447509765625, 34.03472137451172, 25.26987075805664]\n",
      "epoch 8|270; total loss:119.5223159790039\n",
      "last losses: [46.4204216003418, 26.456798553466797, 18.378063201904297]\n",
      "epoch 9|0; total loss:4.242535591125488\n",
      "last losses: [55.89862823486328, 40.49223327636719, 30.885196685791016]\n",
      "epoch 9|30; total loss:98.22101593017578\n",
      "last losses: [31.311973571777344, 25.75609588623047, 16.259315490722656]\n",
      "epoch 9|60; total loss:95.57259368896484\n",
      "last losses: [47.328208923339844, 31.169593811035156, 18.307415008544922]\n",
      "epoch 9|90; total loss:97.61625671386719\n",
      "last losses: [33.74510955810547, 21.52696990966797, 15.130399703979492]\n",
      "epoch 9|120; total loss:98.27214050292969\n",
      "last losses: [37.69902420043945, 26.150676727294922, 22.83568000793457]\n",
      "epoch 9|150; total loss:99.64823150634766\n",
      "last losses: [61.55582046508789, 36.718971252441406, 28.86622428894043]\n",
      "epoch 9|180; total loss:105.08682250976562\n",
      "last losses: [63.955509185791016, 41.62499237060547, 25.934310913085938]\n",
      "epoch 9|210; total loss:102.65060424804688\n",
      "last losses: [55.435306549072266, 35.713600158691406, 20.351430892944336]\n",
      "epoch 9|240; total loss:107.61393737792969\n",
      "last losses: [49.42437744140625, 31.909835815429688, 17.625171661376953]\n",
      "epoch 9|270; total loss:107.44390106201172\n",
      "last losses: [51.72926330566406, 30.87824249267578, 22.31395721435547]\n",
      "epoch 10|0; total loss:2.76576566696167\n",
      "last losses: [33.931785583496094, 30.34052276611328, 18.700660705566406]\n",
      "epoch 10|30; total loss:87.62293243408203\n",
      "last losses: [32.38328552246094, 25.291555404663086, 14.035665512084961]\n",
      "epoch 10|60; total loss:90.49263763427734\n",
      "last losses: [31.078479766845703, 29.83588218688965, 16.409337997436523]\n",
      "epoch 10|90; total loss:91.33136749267578\n",
      "last losses: [41.31050109863281, 36.437660217285156, 21.484622955322266]\n",
      "epoch 10|120; total loss:96.0487060546875\n",
      "last losses: [31.130123138427734, 25.912841796875, 18.5407657623291]\n",
      "epoch 10|150; total loss:89.98422241210938\n",
      "last losses: [33.68648147583008, 28.738019943237305, 16.029624938964844]\n",
      "epoch 10|180; total loss:87.98062133789062\n",
      "last losses: [37.88432312011719, 30.59627914428711, 20.732444763183594]\n",
      "epoch 10|210; total loss:96.42190551757812\n",
      "last losses: [48.26045608520508, 41.38620376586914, 25.719907760620117]\n",
      "epoch 10|240; total loss:96.16777038574219\n",
      "last losses: [53.31529998779297, 36.736568450927734, 29.90518569946289]\n",
      "epoch 10|270; total loss:98.44374084472656\n",
      "last losses: [39.41484451293945, 31.369529724121094, 18.82701873779297]\n",
      "epoch 11|0; total loss:3.6305158138275146\n",
      "last losses: [43.86237335205078, 35.65392303466797, 29.399169921875]\n",
      "epoch 11|30; total loss:81.28784942626953\n",
      "last losses: [33.18363952636719, 32.00550842285156, 19.403358459472656]\n",
      "epoch 11|60; total loss:82.28266143798828\n",
      "last losses: [32.5561408996582, 28.912511825561523, 24.428401947021484]\n",
      "epoch 11|90; total loss:77.37884521484375\n",
      "last losses: [28.0954647064209, 30.23944854736328, 20.113224029541016]\n",
      "epoch 11|120; total loss:81.35272216796875\n",
      "last losses: [35.823760986328125, 29.78754425048828, 19.093788146972656]\n",
      "epoch 11|150; total loss:79.8486557006836\n",
      "last losses: [26.900514602661133, 23.257633209228516, 12.971113204956055]\n",
      "epoch 11|180; total loss:87.3707046508789\n",
      "last losses: [28.159698486328125, 25.232601165771484, 16.456642150878906]\n",
      "epoch 11|210; total loss:89.33356475830078\n",
      "last losses: [39.38349533081055, 35.79938507080078, 23.784286499023438]\n",
      "epoch 11|240; total loss:86.92494201660156\n",
      "last losses: [39.80510711669922, 32.167762756347656, 21.297880172729492]\n",
      "epoch 11|270; total loss:91.63709259033203\n",
      "last losses: [37.72986602783203, 29.991348266601562, 16.316131591796875]\n",
      "epoch 12|0; total loss:2.4458744525909424\n",
      "last losses: [26.871578216552734, 28.55796241760254, 17.946687698364258]\n",
      "epoch 12|30; total loss:76.20618438720703\n",
      "last losses: [31.584598541259766, 29.861597061157227, 22.382150650024414]\n",
      "epoch 12|60; total loss:77.6917953491211\n",
      "last losses: [26.901317596435547, 35.254302978515625, 23.77020835876465]\n",
      "epoch 12|90; total loss:78.8746109008789\n",
      "last losses: [35.4842529296875, 35.716888427734375, 23.174461364746094]\n",
      "epoch 12|120; total loss:79.28372192382812\n",
      "last losses: [32.155845642089844, 33.378536224365234, 20.46758270263672]\n",
      "epoch 12|150; total loss:78.26775360107422\n",
      "last losses: [24.76247787475586, 25.630516052246094, 16.173084259033203]\n",
      "epoch 12|180; total loss:76.80449676513672\n",
      "last losses: [37.07467269897461, 38.4243278503418, 21.720613479614258]\n",
      "epoch 12|210; total loss:80.59225463867188\n",
      "last losses: [29.491722106933594, 30.250789642333984, 20.0142765045166]\n",
      "epoch 12|240; total loss:81.89268493652344\n",
      "last losses: [37.09906768798828, 32.537864685058594, 21.656963348388672]\n",
      "epoch 12|270; total loss:79.66082000732422\n",
      "last losses: [32.50838088989258, 30.13146209716797, 20.289270401000977]\n",
      "epoch 13|0; total loss:2.308418035507202\n",
      "last losses: [24.015491485595703, 27.409215927124023, 17.827835083007812]\n",
      "epoch 13|30; total loss:73.62997436523438\n",
      "last losses: [42.20408248901367, 42.46441650390625, 26.956886291503906]\n",
      "epoch 13|60; total loss:71.48191833496094\n",
      "last losses: [26.199718475341797, 35.51526641845703, 21.924816131591797]\n",
      "epoch 13|90; total loss:71.66898345947266\n",
      "last losses: [16.760133743286133, 20.237213134765625, 12.634628295898438]\n",
      "epoch 13|120; total loss:72.1844711303711\n",
      "last losses: [30.159170150756836, 29.373695373535156, 15.341470718383789]\n",
      "epoch 13|150; total loss:72.00057983398438\n",
      "last losses: [29.807594299316406, 33.19574737548828, 18.50328826904297]\n",
      "epoch 13|180; total loss:80.95220947265625\n",
      "last losses: [25.658428192138672, 27.558916091918945, 18.181753158569336]\n",
      "epoch 13|210; total loss:73.34112548828125\n",
      "last losses: [16.306840896606445, 18.681987762451172, 16.98885154724121]\n",
      "epoch 13|240; total loss:72.88933563232422\n",
      "last losses: [29.90601921081543, 32.87579345703125, 19.30495834350586]\n",
      "epoch 13|270; total loss:77.26349639892578\n",
      "last losses: [24.185428619384766, 26.405601501464844, 16.37332534790039]\n",
      "epoch 14|0; total loss:2.111739158630371\n",
      "last losses: [19.613285064697266, 26.299060821533203, 17.43982696533203]\n",
      "epoch 14|30; total loss:66.32131958007812\n",
      "last losses: [17.9300479888916, 27.142696380615234, 16.16975975036621]\n",
      "epoch 14|60; total loss:67.3745346069336\n",
      "last losses: [23.30229949951172, 24.15093421936035, 16.291065216064453]\n",
      "epoch 14|90; total loss:68.78324127197266\n",
      "last losses: [27.32659912109375, 33.873504638671875, 17.182083129882812]\n",
      "epoch 14|120; total loss:69.5125732421875\n",
      "last losses: [17.98562240600586, 24.31547737121582, 13.184155464172363]\n",
      "epoch 14|150; total loss:72.49934387207031\n",
      "last losses: [38.62960433959961, 37.75330352783203, 25.05411148071289]\n",
      "epoch 14|180; total loss:67.22856903076172\n",
      "last losses: [19.13420295715332, 23.00473976135254, 11.86733627319336]\n",
      "epoch 14|210; total loss:71.66334533691406\n",
      "last losses: [28.352602005004883, 31.769187927246094, 20.152698516845703]\n",
      "epoch 14|240; total loss:68.61481475830078\n",
      "last losses: [25.776714324951172, 30.277645111083984, 17.77899169921875]\n",
      "epoch 14|270; total loss:67.98793029785156\n",
      "last losses: [36.530906677246094, 39.4073371887207, 19.318035125732422]\n",
      "epoch 15|0; total loss:2.1219046115875244\n",
      "last losses: [22.45399284362793, 26.37257957458496, 14.830560684204102]\n",
      "epoch 15|30; total loss:62.79358673095703\n",
      "last losses: [17.151782989501953, 22.56958770751953, 13.21588134765625]\n",
      "epoch 15|60; total loss:65.0442886352539\n",
      "last losses: [23.06057357788086, 30.248228073120117, 17.82093620300293]\n",
      "epoch 15|90; total loss:66.1485366821289\n",
      "last losses: [33.58576202392578, 44.91285705566406, 20.80406379699707]\n",
      "epoch 15|120; total loss:64.3699951171875\n",
      "last losses: [16.88709259033203, 24.8402099609375, 12.359719276428223]\n",
      "epoch 15|150; total loss:67.02764892578125\n",
      "last losses: [28.153053283691406, 33.512001037597656, 20.797969818115234]\n",
      "epoch 15|180; total loss:63.930030822753906\n",
      "last losses: [20.102087020874023, 27.687286376953125, 16.783594131469727]\n",
      "epoch 15|210; total loss:63.90206527709961\n",
      "last losses: [17.06719207763672, 24.674335479736328, 13.520681381225586]\n",
      "epoch 15|240; total loss:67.64584350585938\n",
      "last losses: [27.06162452697754, 36.311546325683594, 23.5281925201416]\n",
      "epoch 15|270; total loss:66.91694641113281\n",
      "last losses: [21.111644744873047, 28.126474380493164, 14.990368843078613]\n",
      "epoch 16|0; total loss:2.1558756828308105\n",
      "last losses: [21.116147994995117, 28.254867553710938, 15.305257797241211]\n",
      "epoch 16|30; total loss:66.32365417480469\n",
      "last losses: [15.917987823486328, 19.54461669921875, 13.996554374694824]\n",
      "epoch 16|60; total loss:57.91397476196289\n",
      "last losses: [17.173765182495117, 29.191789627075195, 16.06079864501953]\n",
      "epoch 16|90; total loss:63.80357360839844\n",
      "last losses: [20.559783935546875, 28.944889068603516, 17.488128662109375]\n",
      "epoch 16|120; total loss:58.62576675415039\n",
      "last losses: [17.9320068359375, 26.942203521728516, 13.588861465454102]\n",
      "epoch 16|150; total loss:60.990318298339844\n",
      "last losses: [21.705509185791016, 27.212158203125, 17.80129623413086]\n",
      "epoch 16|180; total loss:63.18801498413086\n",
      "last losses: [13.359628677368164, 24.905126571655273, 12.09571361541748]\n",
      "epoch 16|210; total loss:65.9826889038086\n",
      "last losses: [24.920886993408203, 28.04042625427246, 12.25425910949707]\n",
      "epoch 16|240; total loss:65.84481811523438\n",
      "last losses: [22.56922721862793, 30.117862701416016, 19.520923614501953]\n",
      "epoch 16|270; total loss:60.79358673095703\n",
      "last losses: [15.678685188293457, 28.11930274963379, 18.20206069946289]\n",
      "epoch 17|0; total loss:1.7836354970932007\n",
      "last losses: [15.503684043884277, 25.17935562133789, 12.82602310180664]\n",
      "epoch 17|30; total loss:57.69472885131836\n",
      "last losses: [14.268508911132812, 24.987409591674805, 13.731151580810547]\n",
      "epoch 17|60; total loss:60.314537048339844\n",
      "last losses: [10.506494522094727, 21.68570327758789, 10.247960090637207]\n",
      "epoch 17|90; total loss:60.776222229003906\n",
      "last losses: [19.873132705688477, 27.291147232055664, 17.027389526367188]\n",
      "epoch 17|120; total loss:59.498722076416016\n",
      "last losses: [17.775283813476562, 27.68649673461914, 14.064151763916016]\n",
      "epoch 17|150; total loss:60.89466857910156\n",
      "last losses: [24.624919891357422, 31.409345626831055, 15.707123756408691]\n",
      "epoch 17|180; total loss:61.2348747253418\n",
      "last losses: [14.354850769042969, 23.60318946838379, 13.709443092346191]\n",
      "epoch 17|210; total loss:59.31914520263672\n",
      "last losses: [24.569786071777344, 34.36296463012695, 15.06406021118164]\n",
      "epoch 17|240; total loss:60.163543701171875\n",
      "last losses: [22.44825553894043, 34.39944839477539, 20.76355743408203]\n",
      "epoch 17|270; total loss:58.64704513549805\n",
      "last losses: [12.893369674682617, 20.493608474731445, 10.64990234375]\n",
      "epoch 18|0; total loss:2.006518840789795\n",
      "last losses: [13.848564147949219, 28.834667205810547, 17.5123291015625]\n",
      "epoch 18|30; total loss:57.58161544799805\n",
      "last losses: [16.176513671875, 27.686084747314453, 16.80400848388672]\n",
      "epoch 18|60; total loss:59.43387222290039\n",
      "last losses: [18.356578826904297, 31.268211364746094, 16.097192764282227]\n",
      "epoch 18|90; total loss:52.09947204589844\n",
      "last losses: [11.281328201293945, 25.859529495239258, 13.339895248413086]\n",
      "epoch 18|120; total loss:58.110469818115234\n",
      "last losses: [19.48627281188965, 33.90930938720703, 16.434574127197266]\n",
      "epoch 18|150; total loss:57.186458587646484\n",
      "last losses: [15.961068153381348, 27.37395668029785, 14.195476531982422]\n",
      "epoch 18|180; total loss:56.1204833984375\n",
      "last losses: [14.78636360168457, 25.331504821777344, 13.924677848815918]\n",
      "epoch 18|210; total loss:55.38993835449219\n",
      "last losses: [16.731870651245117, 24.77574920654297, 14.045019149780273]\n",
      "epoch 18|240; total loss:60.7525520324707\n",
      "last losses: [19.391334533691406, 29.211166381835938, 13.791374206542969]\n",
      "epoch 18|270; total loss:57.71453094482422\n",
      "last losses: [14.567134857177734, 20.274654388427734, 11.16854476928711]\n",
      "epoch 19|0; total loss:2.1889705657958984\n",
      "last losses: [15.631237030029297, 33.97803497314453, 16.059844970703125]\n",
      "epoch 19|30; total loss:59.49061584472656\n",
      "last losses: [12.074681282043457, 22.810033798217773, 10.829277038574219]\n",
      "epoch 19|60; total loss:55.27585983276367\n",
      "last losses: [17.728368759155273, 28.09189224243164, 16.458066940307617]\n",
      "epoch 19|90; total loss:54.22813415527344\n",
      "last losses: [15.353124618530273, 24.864330291748047, 14.039955139160156]\n",
      "epoch 19|120; total loss:54.965118408203125\n",
      "last losses: [11.447179794311523, 21.907020568847656, 10.42599105834961]\n",
      "epoch 19|150; total loss:52.78617477416992\n",
      "last losses: [16.323570251464844, 28.42110252380371, 14.652731895446777]\n",
      "epoch 19|180; total loss:54.85163497924805\n",
      "last losses: [18.61942481994629, 33.78089904785156, 14.915750503540039]\n",
      "epoch 19|210; total loss:56.34852600097656\n",
      "last losses: [13.206409454345703, 23.857128143310547, 12.37719440460205]\n",
      "epoch 19|240; total loss:57.34321975708008\n",
      "last losses: [13.84239387512207, 25.512123107910156, 12.672001838684082]\n",
      "epoch 19|270; total loss:54.4795036315918\n",
      "last losses: [15.666314125061035, 27.300460815429688, 16.654924392700195]\n",
      "epoch 20|0; total loss:1.8815394639968872\n",
      "last losses: [16.98138999938965, 27.1457576751709, 12.319031715393066]\n",
      "epoch 20|30; total loss:55.76278305053711\n",
      "last losses: [18.09768295288086, 31.23318862915039, 16.99553108215332]\n",
      "epoch 20|60; total loss:51.83636474609375\n",
      "last losses: [14.37509536743164, 27.78159523010254, 14.126542091369629]\n",
      "epoch 20|90; total loss:53.902713775634766\n",
      "last losses: [15.903156280517578, 24.81529426574707, 14.405275344848633]\n",
      "epoch 20|120; total loss:51.487449645996094\n",
      "last losses: [10.51521110534668, 23.67281723022461, 15.778440475463867]\n",
      "epoch 20|150; total loss:52.60939025878906\n",
      "last losses: [11.19791030883789, 24.596797943115234, 14.98696517944336]\n",
      "epoch 20|180; total loss:51.219017028808594\n",
      "last losses: [11.596502304077148, 22.84037971496582, 11.681010246276855]\n",
      "epoch 20|210; total loss:56.344730377197266\n",
      "last losses: [20.909578323364258, 29.377933502197266, 13.988370895385742]\n",
      "epoch 20|240; total loss:52.385108947753906\n",
      "last losses: [13.7893648147583, 27.00969696044922, 13.147564888000488]\n",
      "epoch 20|270; total loss:53.89082717895508\n",
      "last losses: [12.9895658493042, 24.33843421936035, 15.431885719299316]\n",
      "epoch 21|0; total loss:2.3028807640075684\n",
      "last losses: [17.044395446777344, 34.156246185302734, 17.88577651977539]\n",
      "epoch 21|30; total loss:52.12126159667969\n",
      "last losses: [12.77278995513916, 25.872825622558594, 13.736223220825195]\n",
      "epoch 21|60; total loss:51.71849060058594\n",
      "last losses: [13.580087661743164, 32.42625427246094, 15.380128860473633]\n",
      "epoch 21|90; total loss:52.18519973754883\n",
      "last losses: [14.759332656860352, 23.07657814025879, 12.74299430847168]\n",
      "epoch 21|120; total loss:52.514156341552734\n",
      "last losses: [21.27104949951172, 29.951244354248047, 17.996726989746094]\n",
      "epoch 21|150; total loss:50.533695220947266\n",
      "last losses: [21.07317352294922, 27.74858856201172, 15.488118171691895]\n",
      "epoch 21|180; total loss:51.202457427978516\n",
      "last losses: [14.317071914672852, 32.427223205566406, 16.23826789855957]\n",
      "epoch 21|210; total loss:51.625099182128906\n",
      "last losses: [14.645721435546875, 27.657264709472656, 13.977925300598145]\n",
      "epoch 21|240; total loss:51.81764602661133\n",
      "last losses: [9.98675537109375, 21.82876205444336, 9.755758285522461]\n",
      "epoch 21|270; total loss:52.8923454284668\n",
      "last losses: [12.49757194519043, 30.51239013671875, 13.676128387451172]\n",
      "epoch 22|0; total loss:1.4413337707519531\n",
      "last losses: [11.409318923950195, 21.031513214111328, 10.799177169799805]\n",
      "epoch 22|30; total loss:52.06617736816406\n",
      "last losses: [9.23472785949707, 19.830535888671875, 10.476134300231934]\n",
      "epoch 22|60; total loss:51.35297775268555\n",
      "last losses: [13.533117294311523, 27.558794021606445, 14.325970649719238]\n",
      "epoch 22|90; total loss:49.54389190673828\n",
      "last losses: [10.328800201416016, 22.92562484741211, 12.594968795776367]\n",
      "epoch 22|120; total loss:51.47896957397461\n",
      "last losses: [11.307126998901367, 23.64205551147461, 12.853044509887695]\n",
      "epoch 22|150; total loss:48.099212646484375\n",
      "last losses: [10.301713943481445, 25.511526107788086, 15.205428123474121]\n",
      "epoch 22|180; total loss:47.626609802246094\n",
      "last losses: [10.155313491821289, 21.55884552001953, 12.40073299407959]\n",
      "epoch 22|210; total loss:47.807621002197266\n",
      "last losses: [10.909122467041016, 25.641429901123047, 12.697065353393555]\n",
      "epoch 22|240; total loss:50.027626037597656\n",
      "last losses: [10.629928588867188, 24.40757942199707, 12.986552238464355]\n",
      "epoch 22|270; total loss:51.55698776245117\n",
      "last losses: [15.896648406982422, 33.4620246887207, 18.721538543701172]\n",
      "epoch 23|0; total loss:1.5856547355651855\n",
      "last losses: [11.398294448852539, 23.12698745727539, 13.04435920715332]\n",
      "epoch 23|30; total loss:48.023338317871094\n",
      "last losses: [10.171794891357422, 21.40573501586914, 10.47616195678711]\n",
      "epoch 23|60; total loss:49.1921501159668\n",
      "last losses: [8.10932731628418, 21.313379287719727, 11.359293937683105]\n",
      "epoch 23|90; total loss:46.85657501220703\n",
      "last losses: [9.983768463134766, 26.139341354370117, 16.216703414916992]\n",
      "epoch 23|120; total loss:47.746395111083984\n",
      "last losses: [9.128331184387207, 22.055089950561523, 10.925678253173828]\n",
      "epoch 23|150; total loss:48.93209457397461\n",
      "last losses: [14.143951416015625, 25.272499084472656, 12.262860298156738]\n",
      "epoch 23|180; total loss:47.874839782714844\n",
      "last losses: [12.107670783996582, 31.030467987060547, 15.599748611450195]\n",
      "epoch 23|210; total loss:47.724937438964844\n",
      "last losses: [8.85537338256836, 20.667448043823242, 9.350418090820312]\n",
      "epoch 23|240; total loss:48.68655014038086\n",
      "last losses: [8.07909870147705, 18.558677673339844, 8.263821601867676]\n",
      "epoch 23|270; total loss:54.095523834228516\n",
      "last losses: [45.13984680175781, 26.662912368774414, 18.050065994262695]\n",
      "epoch 24|0; total loss:1.7794771194458008\n",
      "last losses: [10.245200157165527, 28.050071716308594, 15.08903980255127]\n",
      "epoch 24|30; total loss:46.08647918701172\n",
      "last losses: [7.241949081420898, 18.448251724243164, 8.115213394165039]\n",
      "epoch 24|60; total loss:45.47536849975586\n",
      "last losses: [7.528216361999512, 22.330177307128906, 9.265159606933594]\n",
      "epoch 24|90; total loss:45.43474197387695\n",
      "last losses: [7.439119815826416, 20.106735229492188, 14.453304290771484]\n",
      "epoch 24|120; total loss:45.6256103515625\n",
      "last losses: [10.174848556518555, 26.596532821655273, 12.346729278564453]\n",
      "epoch 24|150; total loss:49.394874572753906\n",
      "last losses: [12.245848655700684, 23.205209732055664, 12.862142562866211]\n",
      "epoch 24|180; total loss:47.744232177734375\n",
      "last losses: [9.17666244506836, 18.54001235961914, 10.405671119689941]\n",
      "epoch 24|210; total loss:46.92537307739258\n",
      "last losses: [10.470024108886719, 25.51964569091797, 14.379831314086914]\n",
      "epoch 24|240; total loss:47.33919906616211\n",
      "last losses: [9.56837272644043, 21.37008285522461, 10.729066848754883]\n",
      "epoch 24|270; total loss:45.44799041748047\n",
      "last losses: [9.075748443603516, 24.92173194885254, 12.098498344421387]\n",
      "epoch 25|0; total loss:1.686514973640442\n",
      "last losses: [9.546609878540039, 26.55467987060547, 14.494156837463379]\n",
      "epoch 25|30; total loss:47.49365997314453\n",
      "last losses: [10.55712890625, 26.501384735107422, 12.987010955810547]\n",
      "epoch 25|60; total loss:44.147865295410156\n",
      "last losses: [10.109284400939941, 25.53693389892578, 14.307343482971191]\n",
      "epoch 25|90; total loss:44.22605514526367\n",
      "last losses: [6.491098880767822, 21.054954528808594, 9.6778564453125]\n",
      "epoch 25|120; total loss:46.58137130737305\n",
      "last losses: [9.702898979187012, 19.97186279296875, 11.182297706604004]\n",
      "epoch 25|150; total loss:47.38346481323242\n",
      "last losses: [7.473663330078125, 22.440214157104492, 11.799836158752441]\n",
      "epoch 25|180; total loss:45.20234680175781\n",
      "last losses: [9.67580795288086, 21.650827407836914, 11.21079158782959]\n",
      "epoch 25|210; total loss:46.22466278076172\n",
      "last losses: [8.570271492004395, 25.101144790649414, 11.145929336547852]\n",
      "epoch 25|240; total loss:45.01850128173828\n",
      "last losses: [9.083542823791504, 22.402908325195312, 10.877471923828125]\n",
      "epoch 25|270; total loss:46.2130012512207\n",
      "last losses: [11.358863830566406, 28.421302795410156, 17.18195343017578]\n",
      "epoch 26|0; total loss:1.3452221155166626\n",
      "last losses: [8.90018081665039, 19.9976863861084, 11.458795547485352]\n",
      "epoch 26|30; total loss:43.89332580566406\n",
      "last losses: [7.189138412475586, 18.186521530151367, 8.558562278747559]\n",
      "epoch 26|60; total loss:42.971500396728516\n",
      "last losses: [10.870065689086914, 22.12676239013672, 10.491766929626465]\n",
      "epoch 26|90; total loss:46.13957977294922\n",
      "last losses: [9.314559936523438, 23.995868682861328, 16.245643615722656]\n",
      "epoch 26|120; total loss:41.092185974121094\n",
      "last losses: [8.945652961730957, 25.922412872314453, 13.376179695129395]\n",
      "epoch 26|150; total loss:44.735694885253906\n",
      "last losses: [8.123475074768066, 25.123077392578125, 11.449613571166992]\n",
      "epoch 26|180; total loss:44.50239562988281\n",
      "last losses: [8.085511207580566, 17.48394012451172, 9.44552993774414]\n",
      "epoch 26|210; total loss:44.13096237182617\n",
      "last losses: [10.609199523925781, 29.131622314453125, 12.57127571105957]\n",
      "epoch 26|240; total loss:44.96039962768555\n",
      "last losses: [8.317299842834473, 23.612049102783203, 10.623497009277344]\n",
      "epoch 26|270; total loss:44.6169319152832\n",
      "last losses: [9.142845153808594, 19.226852416992188, 11.463826179504395]\n",
      "epoch 27|0; total loss:1.435996174812317\n",
      "last losses: [10.244989395141602, 22.92697525024414, 9.907918930053711]\n",
      "epoch 27|30; total loss:42.73594284057617\n",
      "last losses: [10.158259391784668, 22.35334014892578, 12.85269546508789]\n",
      "epoch 27|60; total loss:45.28104782104492\n",
      "last losses: [7.231822967529297, 24.213088989257812, 12.174019813537598]\n",
      "epoch 27|90; total loss:42.25409698486328\n",
      "last losses: [8.37466812133789, 25.175342559814453, 14.531184196472168]\n",
      "epoch 27|120; total loss:41.51669692993164\n",
      "last losses: [10.829183578491211, 23.508167266845703, 11.986759185791016]\n",
      "epoch 27|150; total loss:40.76091384887695\n",
      "last losses: [11.580288887023926, 31.763473510742188, 14.320423126220703]\n",
      "epoch 27|180; total loss:42.81626892089844\n",
      "last losses: [11.052945137023926, 26.27559471130371, 11.330406188964844]\n",
      "epoch 27|210; total loss:43.546653747558594\n",
      "last losses: [8.803919792175293, 27.232086181640625, 13.266722679138184]\n",
      "epoch 27|240; total loss:43.2666130065918\n",
      "last losses: [4.685937881469727, 18.590126037597656, 9.278013229370117]\n",
      "epoch 27|270; total loss:44.37217712402344\n",
      "last losses: [12.076191902160645, 22.878965377807617, 12.289163589477539]\n",
      "epoch 28|0; total loss:1.269797921180725\n",
      "last losses: [8.253780364990234, 21.384727478027344, 8.455430030822754]\n",
      "epoch 28|30; total loss:43.783164978027344\n",
      "last losses: [9.649240493774414, 26.647478103637695, 12.91108512878418]\n",
      "epoch 28|60; total loss:42.31325912475586\n",
      "last losses: [9.531170845031738, 25.9429988861084, 12.521278381347656]\n",
      "epoch 28|90; total loss:41.01144027709961\n",
      "last losses: [5.4741106033325195, 20.47216796875, 10.521614074707031]\n",
      "epoch 28|120; total loss:40.55415344238281\n",
      "last losses: [7.298114776611328, 20.833044052124023, 9.234479904174805]\n",
      "epoch 28|150; total loss:42.270111083984375\n",
      "last losses: [6.316532135009766, 22.779590606689453, 11.133342742919922]\n",
      "epoch 28|180; total loss:39.703887939453125\n",
      "last losses: [4.168172836303711, 22.32777214050293, 10.740836143493652]\n",
      "epoch 28|210; total loss:42.54965591430664\n",
      "last losses: [5.123356819152832, 19.072704315185547, 8.770099639892578]\n",
      "epoch 28|240; total loss:40.45309829711914\n",
      "last losses: [7.899594783782959, 25.59195327758789, 13.337708473205566]\n",
      "epoch 28|270; total loss:41.115047454833984\n",
      "last losses: [8.07483196258545, 24.85906410217285, 12.263855934143066]\n",
      "epoch 29|0; total loss:1.1419122219085693\n",
      "last losses: [7.572897911071777, 18.3739070892334, 8.310562133789062]\n",
      "epoch 29|30; total loss:41.67939758300781\n",
      "last losses: [8.315506935119629, 25.506603240966797, 12.847087860107422]\n",
      "epoch 29|60; total loss:38.34535598754883\n",
      "last losses: [6.891042232513428, 20.91860580444336, 11.637460708618164]\n",
      "epoch 29|90; total loss:39.323219299316406\n",
      "last losses: [6.399030685424805, 19.978797912597656, 8.678079605102539]\n",
      "epoch 29|120; total loss:38.90994644165039\n",
      "last losses: [7.1404619216918945, 20.355972290039062, 9.242655754089355]\n",
      "epoch 29|150; total loss:42.62702941894531\n",
      "last losses: [5.353351593017578, 25.260501861572266, 13.117440223693848]\n",
      "epoch 29|180; total loss:41.950279235839844\n",
      "last losses: [7.701316833496094, 19.923208236694336, 11.1494140625]\n",
      "epoch 29|210; total loss:40.177181243896484\n",
      "last losses: [8.069576263427734, 21.262645721435547, 10.885274887084961]\n",
      "epoch 29|240; total loss:41.21513366699219\n",
      "last losses: [8.997657775878906, 21.105186462402344, 10.863214492797852]\n",
      "epoch 29|270; total loss:40.85744857788086\n",
      "last losses: [6.169626235961914, 17.63658905029297, 7.759183406829834]\n",
      "epoch 30|0; total loss:1.2203880548477173\n",
      "last losses: [5.849026203155518, 21.461341857910156, 9.301273345947266]\n",
      "epoch 30|30; total loss:39.10816955566406\n",
      "last losses: [9.51772689819336, 19.578571319580078, 10.015027046203613]\n",
      "epoch 30|60; total loss:38.79280090332031\n",
      "last losses: [10.627006530761719, 23.692176818847656, 10.351161003112793]\n",
      "epoch 30|90; total loss:38.14863586425781\n",
      "last losses: [6.011222839355469, 20.568572998046875, 9.193593978881836]\n",
      "epoch 30|120; total loss:39.59087371826172\n",
      "last losses: [8.5422945022583, 20.789493560791016, 11.43659782409668]\n",
      "epoch 30|150; total loss:38.94203186035156\n",
      "last losses: [8.712010383605957, 22.682308197021484, 10.77294635772705]\n",
      "epoch 30|180; total loss:39.48822784423828\n",
      "last losses: [9.360092163085938, 27.176761627197266, 14.001851081848145]\n",
      "epoch 30|210; total loss:40.61863708496094\n",
      "last losses: [6.41062593460083, 21.079374313354492, 12.286113739013672]\n",
      "epoch 30|240; total loss:38.41135025024414\n",
      "last losses: [6.576974391937256, 21.83307647705078, 11.132060050964355]\n",
      "epoch 30|270; total loss:39.52138900756836\n",
      "last losses: [8.35677719116211, 23.60860824584961, 11.659530639648438]\n",
      "epoch 31|0; total loss:1.4043116569519043\n",
      "last losses: [6.021355628967285, 24.905458450317383, 11.202537536621094]\n",
      "epoch 31|30; total loss:38.632286071777344\n",
      "last losses: [5.9859161376953125, 18.808265686035156, 9.851686477661133]\n",
      "epoch 31|60; total loss:36.979549407958984\n",
      "last losses: [6.017578125, 18.271347045898438, 9.551895141601562]\n",
      "epoch 31|90; total loss:37.15990447998047\n",
      "last losses: [7.316283226013184, 19.202857971191406, 9.38622760772705]\n",
      "epoch 31|120; total loss:37.647918701171875\n",
      "last losses: [7.346705436706543, 24.537878036499023, 12.634782791137695]\n",
      "epoch 31|150; total loss:38.561241149902344\n",
      "last losses: [4.881235122680664, 15.876313209533691, 8.222858428955078]\n",
      "epoch 31|180; total loss:38.08183288574219\n",
      "last losses: [6.38831090927124, 19.238500595092773, 9.555818557739258]\n",
      "epoch 31|210; total loss:37.04743194580078\n",
      "last losses: [5.752771377563477, 21.569242477416992, 10.547348022460938]\n",
      "epoch 31|240; total loss:37.3020133972168\n",
      "last losses: [5.5833964347839355, 18.798625946044922, 11.31695556640625]\n",
      "epoch 31|270; total loss:38.42611312866211\n",
      "last losses: [7.950098991394043, 20.39830207824707, 11.06849479675293]\n",
      "epoch 32|0; total loss:1.0783504247665405\n",
      "last losses: [4.856463432312012, 17.553569793701172, 9.940475463867188]\n",
      "epoch 32|30; total loss:36.64659881591797\n",
      "last losses: [5.115987777709961, 19.28520393371582, 8.591450691223145]\n",
      "epoch 32|60; total loss:36.15199279785156\n",
      "last losses: [5.906101703643799, 22.711910247802734, 12.13792896270752]\n",
      "epoch 32|90; total loss:37.77071762084961\n",
      "last losses: [6.904933929443359, 20.33399200439453, 10.824185371398926]\n",
      "epoch 32|120; total loss:38.699466705322266\n",
      "last losses: [5.831850051879883, 22.544349670410156, 10.151853561401367]\n",
      "epoch 32|150; total loss:38.37358093261719\n",
      "last losses: [6.592229843139648, 24.4229793548584, 10.310992240905762]\n",
      "epoch 32|180; total loss:35.378292083740234\n",
      "last losses: [6.602344036102295, 21.75347137451172, 10.31348991394043]\n",
      "epoch 32|210; total loss:37.49932098388672\n",
      "last losses: [5.22709321975708, 19.57977867126465, 8.688248634338379]\n",
      "epoch 32|240; total loss:39.57888412475586\n",
      "last losses: [7.201458930969238, 22.117328643798828, 11.879133224487305]\n",
      "epoch 32|270; total loss:37.656654357910156\n",
      "last losses: [4.4639506340026855, 20.054712295532227, 9.523778915405273]\n",
      "epoch 33|0; total loss:1.4286820888519287\n",
      "last losses: [8.999056816101074, 23.202911376953125, 10.65849494934082]\n",
      "epoch 33|30; total loss:36.93769073486328\n",
      "last losses: [5.29974365234375, 19.461225509643555, 9.687230110168457]\n",
      "epoch 33|60; total loss:35.98066329956055\n",
      "last losses: [3.9402754306793213, 19.633060455322266, 9.380444526672363]\n",
      "epoch 33|90; total loss:38.093780517578125\n",
      "last losses: [6.510584354400635, 19.02362060546875, 10.331842422485352]\n",
      "epoch 33|120; total loss:36.80757141113281\n",
      "last losses: [3.0340089797973633, 14.711562156677246, 7.594425678253174]\n",
      "epoch 33|150; total loss:36.155426025390625\n",
      "last losses: [5.057506561279297, 23.31649398803711, 11.390968322753906]\n",
      "epoch 33|180; total loss:36.656314849853516\n",
      "last losses: [6.599405288696289, 23.827106475830078, 11.784749031066895]\n",
      "epoch 33|210; total loss:36.96433639526367\n",
      "last losses: [5.64335298538208, 19.265132904052734, 8.85556411743164]\n",
      "epoch 33|240; total loss:39.0554084777832\n",
      "last losses: [6.825799465179443, 21.329896926879883, 11.397303581237793]\n",
      "epoch 33|270; total loss:36.912437438964844\n",
      "last losses: [5.326069355010986, 18.730987548828125, 9.097586631774902]\n",
      "epoch 34|0; total loss:1.445888638496399\n",
      "last losses: [6.767672061920166, 22.813432693481445, 13.795548439025879]\n",
      "epoch 34|30; total loss:33.802635192871094\n",
      "last losses: [7.1036248207092285, 20.115650177001953, 8.793729782104492]\n",
      "epoch 34|60; total loss:37.00234603881836\n",
      "last losses: [5.180728435516357, 23.079750061035156, 10.885891914367676]\n",
      "epoch 34|90; total loss:34.57524108886719\n",
      "last losses: [4.558870315551758, 16.3515682220459, 9.918011665344238]\n",
      "epoch 34|120; total loss:34.92799377441406\n",
      "last losses: [8.35014820098877, 27.14005470275879, 12.489160537719727]\n",
      "epoch 34|150; total loss:36.66604232788086\n",
      "last losses: [5.338021755218506, 18.810914993286133, 9.495047569274902]\n",
      "epoch 34|180; total loss:35.24196243286133\n",
      "last losses: [4.095853805541992, 16.61678123474121, 7.7335968017578125]\n",
      "epoch 34|210; total loss:36.09739303588867\n",
      "last losses: [5.119144439697266, 18.659303665161133, 9.252155303955078]\n",
      "epoch 34|240; total loss:35.79007339477539\n",
      "last losses: [6.622195720672607, 17.407739639282227, 9.408783912658691]\n",
      "epoch 34|270; total loss:35.48409652709961\n",
      "last losses: [7.666055202484131, 22.70969581604004, 10.602957725524902]\n",
      "epoch 35|0; total loss:1.1970891952514648\n",
      "last losses: [5.594027519226074, 20.093231201171875, 10.22541618347168]\n",
      "epoch 35|30; total loss:34.01884841918945\n",
      "last losses: [5.893832683563232, 20.01885986328125, 11.157830238342285]\n",
      "epoch 35|60; total loss:35.922691345214844\n",
      "last losses: [3.9099597930908203, 17.879806518554688, 8.432327270507812]\n",
      "epoch 35|90; total loss:34.626426696777344\n",
      "last losses: [4.726232051849365, 13.721309661865234, 6.823565483093262]\n",
      "epoch 35|120; total loss:34.574825286865234\n",
      "last losses: [3.1714353561401367, 13.41341781616211, 6.848965644836426]\n",
      "epoch 35|150; total loss:35.81934356689453\n",
      "last losses: [2.636586904525757, 18.954936981201172, 9.477782249450684]\n",
      "epoch 35|180; total loss:33.85783004760742\n",
      "last losses: [4.8194708824157715, 16.80317497253418, 8.493563652038574]\n",
      "epoch 35|210; total loss:35.14847946166992\n",
      "last losses: [5.355748653411865, 23.020261764526367, 13.905461311340332]\n",
      "epoch 35|240; total loss:33.10432052612305\n",
      "last losses: [4.7425537109375, 14.956875801086426, 8.859079360961914]\n",
      "epoch 35|270; total loss:35.272438049316406\n",
      "last losses: [4.304773330688477, 18.66351318359375, 9.23326301574707]\n",
      "epoch 36|0; total loss:1.5654836893081665\n",
      "last losses: [5.353586196899414, 28.26388931274414, 13.347031593322754]\n",
      "epoch 36|30; total loss:33.930763244628906\n",
      "last losses: [4.277024269104004, 14.054384231567383, 6.977250099182129]\n",
      "epoch 36|60; total loss:34.673805236816406\n",
      "last losses: [4.268733978271484, 17.860023498535156, 9.5382080078125]\n",
      "epoch 36|90; total loss:34.59205627441406\n",
      "last losses: [7.009858131408691, 20.993873596191406, 11.927459716796875]\n",
      "epoch 36|120; total loss:33.232425689697266\n",
      "last losses: [5.79043436050415, 14.27107048034668, 6.975213050842285]\n",
      "epoch 36|150; total loss:33.15724182128906\n",
      "last losses: [4.709553241729736, 19.417835235595703, 10.456594467163086]\n",
      "epoch 36|180; total loss:33.378173828125\n",
      "last losses: [5.22786808013916, 19.039669036865234, 10.907570838928223]\n",
      "epoch 36|210; total loss:34.52306365966797\n",
      "last losses: [6.84788179397583, 20.173091888427734, 13.123555183410645]\n",
      "epoch 36|240; total loss:34.4876708984375\n",
      "last losses: [5.0132927894592285, 20.829214096069336, 12.017412185668945]\n",
      "epoch 36|270; total loss:34.37203598022461\n",
      "last losses: [4.600659370422363, 21.305265426635742, 9.341538429260254]\n",
      "epoch 37|0; total loss:0.9886242151260376\n",
      "last losses: [3.2064309120178223, 16.88971710205078, 9.562577247619629]\n",
      "epoch 37|30; total loss:33.13026428222656\n",
      "last losses: [3.409390449523926, 14.993477821350098, 7.338315963745117]\n",
      "epoch 37|60; total loss:32.33864974975586\n",
      "last losses: [6.31470251083374, 19.59051513671875, 10.094245910644531]\n",
      "epoch 37|90; total loss:33.87592315673828\n",
      "last losses: [3.946368932723999, 17.134525299072266, 8.014150619506836]\n",
      "epoch 37|120; total loss:34.56999206542969\n",
      "last losses: [5.137294292449951, 17.033618927001953, 8.941298484802246]\n",
      "epoch 37|150; total loss:33.430809020996094\n",
      "last losses: [3.841958999633789, 17.42694854736328, 8.497754096984863]\n",
      "epoch 37|180; total loss:30.995201110839844\n",
      "last losses: [2.350032329559326, 15.945955276489258, 7.96914529800415]\n",
      "epoch 37|210; total loss:32.7945671081543\n",
      "last losses: [4.931046485900879, 16.01961898803711, 9.269883155822754]\n",
      "epoch 37|240; total loss:31.967288970947266\n",
      "last losses: [4.421874046325684, 17.644752502441406, 8.976407051086426]\n",
      "epoch 37|270; total loss:33.02064895629883\n",
      "last losses: [3.841041326522827, 19.888343811035156, 10.677538871765137]\n",
      "epoch 38|0; total loss:1.0647377967834473\n",
      "last losses: [4.40143346786499, 17.649810791015625, 9.890889167785645]\n",
      "epoch 38|30; total loss:31.47685432434082\n",
      "last losses: [3.8739376068115234, 18.615734100341797, 10.180810928344727]\n",
      "epoch 38|60; total loss:31.41593360900879\n",
      "last losses: [4.7640790939331055, 23.52469825744629, 12.813774108886719]\n",
      "epoch 38|90; total loss:31.100191116333008\n",
      "last losses: [5.927585601806641, 25.79371452331543, 12.560028076171875]\n",
      "epoch 38|120; total loss:31.632177352905273\n",
      "last losses: [4.343825340270996, 18.24417495727539, 7.995291709899902]\n",
      "epoch 38|150; total loss:33.32722473144531\n",
      "last losses: [7.1972527503967285, 19.25248908996582, 10.719843864440918]\n",
      "epoch 38|180; total loss:31.462921142578125\n",
      "last losses: [6.935860633850098, 15.790083885192871, 10.27469539642334]\n",
      "epoch 38|210; total loss:34.47276306152344\n",
      "last losses: [4.977595329284668, 20.237314224243164, 9.783016204833984]\n",
      "epoch 38|240; total loss:32.503578186035156\n",
      "last losses: [6.743394374847412, 16.58468246459961, 9.121511459350586]\n",
      "epoch 38|270; total loss:32.71624755859375\n",
      "last losses: [5.040931701660156, 22.588897705078125, 11.507494926452637]\n",
      "epoch 39|0; total loss:1.1061872243881226\n",
      "last losses: [4.849126815795898, 18.108064651489258, 10.228425025939941]\n",
      "epoch 39|30; total loss:30.188692092895508\n",
      "last losses: [5.084194660186768, 20.62399673461914, 8.697628021240234]\n",
      "epoch 39|60; total loss:31.113279342651367\n",
      "last losses: [4.525416374206543, 15.910507202148438, 9.361198425292969]\n",
      "epoch 39|90; total loss:32.15953063964844\n",
      "last losses: [3.818873405456543, 18.277374267578125, 9.558023452758789]\n",
      "epoch 39|120; total loss:30.747434616088867\n",
      "last losses: [4.1870527267456055, 21.34514617919922, 11.812492370605469]\n",
      "epoch 39|150; total loss:32.50503158569336\n",
      "last losses: [5.771135330200195, 19.716224670410156, 10.184235572814941]\n",
      "epoch 39|180; total loss:31.143054962158203\n",
      "last losses: [8.147403717041016, 19.176250457763672, 10.391162872314453]\n",
      "epoch 39|210; total loss:31.542644500732422\n",
      "last losses: [3.890230417251587, 12.456942558288574, 7.557389259338379]\n",
      "epoch 39|240; total loss:33.76380157470703\n",
      "last losses: [3.7223830223083496, 22.372482299804688, 10.209551811218262]\n",
      "epoch 39|270; total loss:31.253292083740234\n",
      "last losses: [2.883228063583374, 16.18636703491211, 8.31348991394043]\n",
      "epoch 40|0; total loss:1.2495659589767456\n",
      "last losses: [5.359291076660156, 20.3330078125, 11.794678688049316]\n",
      "epoch 40|30; total loss:31.1728458404541\n",
      "last losses: [4.737770080566406, 17.09099769592285, 9.040692329406738]\n",
      "epoch 40|60; total loss:31.355850219726562\n",
      "last losses: [4.185256481170654, 18.79693031311035, 11.93766975402832]\n",
      "epoch 40|90; total loss:30.63214683532715\n",
      "last losses: [2.3978023529052734, 14.4489164352417, 7.489535331726074]\n",
      "epoch 40|120; total loss:29.4425048828125\n",
      "last losses: [3.606333017349243, 16.220670700073242, 9.061119079589844]\n",
      "epoch 40|150; total loss:29.873111724853516\n",
      "last losses: [5.6580657958984375, 23.38064956665039, 13.437243461608887]\n",
      "epoch 40|180; total loss:31.98918342590332\n",
      "last losses: [4.095233917236328, 16.737136840820312, 8.943058967590332]\n",
      "epoch 40|210; total loss:30.164928436279297\n",
      "last losses: [2.907846450805664, 18.595617294311523, 9.928827285766602]\n",
      "epoch 40|240; total loss:29.04643440246582\n",
      "last losses: [4.182057857513428, 17.185686111450195, 8.969346046447754]\n",
      "epoch 40|270; total loss:29.9290714263916\n",
      "last losses: [4.484101295471191, 18.866069793701172, 10.707186698913574]\n",
      "epoch 41|0; total loss:1.1402240991592407\n",
      "last losses: [4.654584884643555, 19.234272003173828, 10.317865371704102]\n",
      "epoch 41|30; total loss:29.300968170166016\n",
      "last losses: [4.452701091766357, 14.384967803955078, 8.364347457885742]\n",
      "epoch 41|60; total loss:30.315767288208008\n",
      "last losses: [2.514329195022583, 15.134353637695312, 8.668371200561523]\n",
      "epoch 41|90; total loss:29.15296745300293\n",
      "last losses: [4.3425188064575195, 17.15220832824707, 8.629093170166016]\n",
      "epoch 41|120; total loss:29.122488021850586\n",
      "last losses: [2.774685859680176, 12.773838996887207, 5.984357833862305]\n",
      "epoch 41|150; total loss:28.88478660583496\n",
      "last losses: [3.0050933361053467, 14.103662490844727, 7.666726589202881]\n",
      "epoch 41|180; total loss:28.822309494018555\n",
      "last losses: [5.978884696960449, 18.800992965698242, 11.142034530639648]\n",
      "epoch 41|210; total loss:30.164663314819336\n",
      "last losses: [4.464616298675537, 15.538411140441895, 11.019294738769531]\n",
      "epoch 41|240; total loss:30.580425262451172\n",
      "last losses: [3.1842241287231445, 15.204233169555664, 8.105957984924316]\n",
      "epoch 41|270; total loss:30.80898666381836\n",
      "last losses: [2.814683198928833, 16.37636375427246, 9.436332702636719]\n",
      "epoch 42|0; total loss:1.03355073928833\n",
      "last losses: [3.8191661834716797, 16.956378936767578, 10.230976104736328]\n",
      "epoch 42|30; total loss:29.373167037963867\n",
      "last losses: [2.6869473457336426, 13.893190383911133, 8.093631744384766]\n",
      "epoch 42|60; total loss:29.278854370117188\n",
      "last losses: [2.8772881031036377, 13.724257469177246, 8.301356315612793]\n",
      "epoch 42|90; total loss:30.802894592285156\n",
      "last losses: [4.702620029449463, 16.463577270507812, 9.373151779174805]\n",
      "epoch 42|120; total loss:27.80558967590332\n",
      "last losses: [4.099086761474609, 16.060192108154297, 8.630781173706055]\n",
      "epoch 42|150; total loss:28.656713485717773\n",
      "last losses: [4.47989559173584, 24.18186378479004, 12.06592845916748]\n",
      "epoch 42|180; total loss:29.951894760131836\n",
      "last losses: [4.594788074493408, 17.848405838012695, 8.164834976196289]\n",
      "epoch 42|210; total loss:29.394107818603516\n",
      "last losses: [3.9405975341796875, 13.584239959716797, 9.01052188873291]\n",
      "epoch 42|240; total loss:27.7657527923584\n",
      "last losses: [5.013363838195801, 16.51757049560547, 8.284343719482422]\n",
      "epoch 42|270; total loss:29.060148239135742\n",
      "last losses: [2.4947283267974854, 15.702120780944824, 8.885467529296875]\n",
      "epoch 43|0; total loss:0.6804834604263306\n",
      "last losses: [2.3166489601135254, 12.577226638793945, 5.520627021789551]\n",
      "epoch 43|30; total loss:28.594467163085938\n",
      "last losses: [5.973065376281738, 18.94523048400879, 9.507479667663574]\n",
      "epoch 43|60; total loss:26.851226806640625\n",
      "last losses: [2.3106656074523926, 15.339813232421875, 8.32325553894043]\n",
      "epoch 43|90; total loss:27.255868911743164\n",
      "last losses: [2.000730276107788, 14.028074264526367, 7.792902946472168]\n",
      "epoch 43|120; total loss:28.61368751525879\n",
      "last losses: [2.6766648292541504, 14.893011093139648, 8.277276039123535]\n",
      "epoch 43|150; total loss:27.770313262939453\n",
      "last losses: [3.183856964111328, 16.774511337280273, 9.026924133300781]\n",
      "epoch 43|180; total loss:28.073549270629883\n",
      "last losses: [4.558053493499756, 15.454166412353516, 8.707338333129883]\n",
      "epoch 43|210; total loss:29.025442123413086\n",
      "last losses: [2.6756467819213867, 15.443632125854492, 7.471446990966797]\n",
      "epoch 43|240; total loss:29.11967658996582\n",
      "last losses: [4.588470458984375, 14.833569526672363, 6.6375579833984375]\n",
      "epoch 43|270; total loss:30.641212463378906\n",
      "last losses: [2.3052544593811035, 16.712888717651367, 8.441495895385742]\n",
      "epoch 44|0; total loss:0.8011864423751831\n",
      "last losses: [1.9155921936035156, 14.308324813842773, 7.811675071716309]\n",
      "epoch 44|30; total loss:28.65834617614746\n",
      "last losses: [4.164279937744141, 15.462742805480957, 8.92236614227295]\n",
      "epoch 44|60; total loss:27.548755645751953\n",
      "last losses: [5.079372882843018, 15.721380233764648, 8.37408447265625]\n",
      "epoch 44|90; total loss:28.235050201416016\n",
      "last losses: [4.540037631988525, 14.500658988952637, 7.771556854248047]\n",
      "epoch 44|120; total loss:27.667186737060547\n",
      "last losses: [3.9464550018310547, 17.80398178100586, 11.128358840942383]\n",
      "epoch 44|150; total loss:26.927072525024414\n",
      "last losses: [2.6408638954162598, 15.2379789352417, 7.6490797996521]\n",
      "epoch 44|180; total loss:27.38909912109375\n",
      "last losses: [2.745297431945801, 13.304956436157227, 6.640971660614014]\n",
      "epoch 44|210; total loss:27.301759719848633\n",
      "last losses: [5.553722381591797, 17.31255340576172, 9.029749870300293]\n",
      "epoch 44|240; total loss:29.316600799560547\n",
      "last losses: [2.949188470840454, 14.838735580444336, 9.609450340270996]\n",
      "epoch 44|270; total loss:27.844411849975586\n",
      "last losses: [1.903371810913086, 14.502471923828125, 9.20751667022705]\n",
      "epoch 45|0; total loss:0.6866784691810608\n",
      "last losses: [0.9098563194274902, 11.967057228088379, 7.7234392166137695]\n",
      "epoch 45|30; total loss:26.95233726501465\n",
      "last losses: [3.8923628330230713, 14.784700393676758, 10.551121711730957]\n",
      "epoch 45|60; total loss:27.067901611328125\n",
      "last losses: [2.3437724113464355, 13.752958297729492, 7.790020942687988]\n",
      "epoch 45|90; total loss:27.549795150756836\n",
      "last losses: [2.5718252658843994, 14.653251647949219, 8.028669357299805]\n",
      "epoch 45|120; total loss:27.228822708129883\n",
      "last losses: [1.8248733282089233, 10.310044288635254, 7.097796440124512]\n",
      "epoch 45|150; total loss:26.83978843688965\n",
      "last losses: [2.484198570251465, 15.977928161621094, 11.239811897277832]\n",
      "epoch 45|180; total loss:27.20712661743164\n",
      "last losses: [3.2998509407043457, 16.33951187133789, 10.281530380249023]\n",
      "epoch 45|210; total loss:27.531757354736328\n",
      "last losses: [2.471036195755005, 13.614837646484375, 8.741004943847656]\n",
      "epoch 45|240; total loss:26.985307693481445\n",
      "last losses: [2.616594076156616, 17.83864974975586, 9.588191986083984]\n",
      "epoch 45|270; total loss:27.764738082885742\n",
      "last losses: [3.0268476009368896, 14.144230842590332, 7.810023784637451]\n",
      "epoch 46|0; total loss:0.8806904554367065\n",
      "last losses: [3.2929847240448, 13.684463500976562, 9.443262100219727]\n",
      "epoch 46|30; total loss:25.99359893798828\n",
      "last losses: [2.4895918369293213, 14.512567520141602, 8.511589050292969]\n",
      "epoch 46|60; total loss:26.069643020629883\n",
      "last losses: [2.399487018585205, 15.951677322387695, 9.315608978271484]\n",
      "epoch 46|90; total loss:26.013381958007812\n",
      "last losses: [2.365511417388916, 13.890965461730957, 7.124875068664551]\n",
      "epoch 46|120; total loss:27.735260009765625\n",
      "last losses: [2.7746810913085938, 15.296605110168457, 9.259886741638184]\n",
      "epoch 46|150; total loss:27.449352264404297\n",
      "last losses: [2.6945037841796875, 16.07828140258789, 9.755226135253906]\n",
      "epoch 46|180; total loss:28.543672561645508\n",
      "last losses: [2.924898862838745, 14.41572093963623, 9.619270324707031]\n",
      "epoch 46|210; total loss:28.148893356323242\n",
      "last losses: [3.8629543781280518, 13.908061981201172, 8.686427116394043]\n",
      "epoch 46|240; total loss:26.79336929321289\n",
      "last losses: [3.816530466079712, 11.686077117919922, 8.050052642822266]\n",
      "epoch 46|270; total loss:27.23038101196289\n",
      "last losses: [2.9536685943603516, 15.011601448059082, 8.46596908569336]\n",
      "epoch 47|0; total loss:0.7716480493545532\n",
      "last losses: [1.7243000268936157, 13.632551193237305, 7.79258918762207]\n",
      "epoch 47|30; total loss:27.0775203704834\n",
      "last losses: [1.3303401470184326, 12.853513717651367, 7.252569675445557]\n",
      "epoch 47|60; total loss:25.451892852783203\n",
      "last losses: [1.8613086938858032, 12.99174690246582, 10.055733680725098]\n",
      "epoch 47|90; total loss:26.97475814819336\n",
      "last losses: [2.842515230178833, 16.977060317993164, 16.34634780883789]\n",
      "epoch 47|120; total loss:26.43196678161621\n",
      "last losses: [3.738096237182617, 13.822916030883789, 6.518856525421143]\n",
      "epoch 47|150; total loss:26.497892379760742\n",
      "last losses: [2.0429115295410156, 12.903103828430176, 7.373076438903809]\n",
      "epoch 47|180; total loss:26.058645248413086\n",
      "last losses: [2.2946550846099854, 13.27692699432373, 7.9030537605285645]\n",
      "epoch 47|210; total loss:25.623929977416992\n",
      "last losses: [1.1587796211242676, 12.366600036621094, 6.44204044342041]\n",
      "epoch 47|240; total loss:25.266565322875977\n",
      "last losses: [2.884840488433838, 13.510675430297852, 7.974142551422119]\n",
      "epoch 47|270; total loss:27.749168395996094\n",
      "last losses: [3.5022659301757812, 15.301319122314453, 8.448796272277832]\n",
      "epoch 48|0; total loss:0.7031541466712952\n",
      "last losses: [1.310447335243225, 12.357154846191406, 7.4270219802856445]\n",
      "epoch 48|30; total loss:25.043933868408203\n",
      "last losses: [3.3041038513183594, 17.64345932006836, 10.456703186035156]\n",
      "epoch 48|60; total loss:25.221464157104492\n",
      "last losses: [5.531705856323242, 14.81074047088623, 8.50573444366455]\n",
      "epoch 48|90; total loss:25.148168563842773\n",
      "last losses: [3.678190231323242, 13.233015060424805, 8.397830963134766]\n",
      "epoch 48|120; total loss:26.12971305847168\n",
      "last losses: [1.4450552463531494, 10.495848655700684, 7.135797023773193]\n",
      "epoch 48|150; total loss:26.66413688659668\n",
      "last losses: [3.4362664222717285, 17.972288131713867, 9.756455421447754]\n",
      "epoch 48|180; total loss:25.936626434326172\n",
      "last losses: [4.688628196716309, 18.53952407836914, 11.867278099060059]\n",
      "epoch 48|210; total loss:25.052854537963867\n",
      "last losses: [2.5735108852386475, 14.003948211669922, 8.264065742492676]\n",
      "epoch 48|240; total loss:24.307180404663086\n",
      "last losses: [2.4702658653259277, 14.79903793334961, 8.97000789642334]\n",
      "epoch 48|270; total loss:24.545608520507812\n",
      "last losses: [1.0832690000534058, 12.359399795532227, 7.442378044128418]\n",
      "epoch 49|0; total loss:0.6949907541275024\n",
      "last losses: [2.139030933380127, 12.484792709350586, 6.225897789001465]\n",
      "epoch 49|30; total loss:25.165283203125\n",
      "last losses: [1.5820388793945312, 13.764566421508789, 7.90753698348999]\n",
      "epoch 49|60; total loss:23.389657974243164\n",
      "last losses: [1.52534818649292, 12.06901741027832, 9.775458335876465]\n",
      "epoch 49|90; total loss:24.77465057373047\n",
      "last losses: [1.742838978767395, 11.834089279174805, 8.235488891601562]\n",
      "epoch 49|120; total loss:23.785669326782227\n",
      "last losses: [2.423853874206543, 15.425324440002441, 9.638933181762695]\n",
      "epoch 49|150; total loss:25.869626998901367\n",
      "last losses: [1.2808036804199219, 13.038275718688965, 7.251472473144531]\n",
      "epoch 49|180; total loss:25.42024803161621\n",
      "last losses: [2.3923933506011963, 16.76079559326172, 9.521501541137695]\n",
      "epoch 49|210; total loss:25.88309669494629\n",
      "last losses: [2.0230376720428467, 12.140462875366211, 8.085291862487793]\n",
      "epoch 49|240; total loss:25.092086791992188\n",
      "last losses: [2.59586763381958, 13.61645221710205, 10.427726745605469]\n",
      "epoch 49|270; total loss:25.282167434692383\n",
      "last losses: [2.4223520755767822, 16.97292709350586, 7.489572525024414]\n",
      "epoch 50|0; total loss:0.9570868015289307\n",
      "last losses: [1.9323275089263916, 17.765392303466797, 9.014883041381836]\n",
      "epoch 50|30; total loss:23.0640811920166\n",
      "last losses: [2.1265413761138916, 15.126164436340332, 9.724475860595703]\n",
      "epoch 50|60; total loss:24.271141052246094\n",
      "last losses: [3.8642618656158447, 13.77060317993164, 7.967782974243164]\n",
      "epoch 50|90; total loss:24.04079246520996\n",
      "last losses: [3.5075736045837402, 15.469131469726562, 8.841700553894043]\n",
      "epoch 50|120; total loss:22.628150939941406\n",
      "last losses: [1.7633806467056274, 11.69870376586914, 6.5167460441589355]\n",
      "epoch 50|150; total loss:24.52890968322754\n",
      "last losses: [1.4896626472473145, 11.798015594482422, 6.271283149719238]\n",
      "epoch 50|180; total loss:24.50818634033203\n",
      "last losses: [3.147890567779541, 12.568000793457031, 6.771076202392578]\n",
      "epoch 50|210; total loss:24.367761611938477\n",
      "last losses: [1.8451910018920898, 14.162984848022461, 8.125555992126465]\n",
      "epoch 50|240; total loss:23.975168228149414\n",
      "last losses: [2.9044456481933594, 13.837967872619629, 8.512743949890137]\n",
      "epoch 50|270; total loss:25.693622589111328\n",
      "last losses: [3.125251054763794, 14.765597343444824, 7.960875511169434]\n",
      "epoch 51|0; total loss:0.8654875159263611\n",
      "last losses: [3.2347264289855957, 13.244546890258789, 9.4853515625]\n",
      "epoch 51|30; total loss:24.397689819335938\n",
      "last losses: [4.179805755615234, 12.435348510742188, 7.789394378662109]\n",
      "epoch 51|60; total loss:23.495193481445312\n",
      "last losses: [1.8143985271453857, 13.08568000793457, 6.8309831619262695]\n",
      "epoch 51|90; total loss:23.49702262878418\n",
      "last losses: [2.528042793273926, 14.819503784179688, 8.726371765136719]\n",
      "epoch 51|120; total loss:24.187040328979492\n",
      "last losses: [2.442549228668213, 12.28176498413086, 7.5099334716796875]\n",
      "epoch 51|150; total loss:22.823421478271484\n",
      "last losses: [2.016404628753662, 15.200248718261719, 9.409194946289062]\n",
      "epoch 51|180; total loss:23.93614387512207\n",
      "last losses: [1.1608648300170898, 11.542725563049316, 7.046175956726074]\n",
      "epoch 51|210; total loss:24.15614128112793\n",
      "last losses: [2.221916437149048, 15.303489685058594, 8.961586952209473]\n",
      "epoch 51|240; total loss:24.080636978149414\n",
      "last losses: [1.9552433490753174, 11.295858383178711, 6.3490800857543945]\n",
      "epoch 51|270; total loss:24.237079620361328\n",
      "last losses: [5.58696985244751, 18.22205352783203, 10.043428421020508]\n",
      "epoch 52|0; total loss:0.9522305130958557\n",
      "last losses: [2.1073780059814453, 16.787216186523438, 9.672320365905762]\n",
      "epoch 52|30; total loss:24.54799461364746\n",
      "last losses: [1.7651461362838745, 11.269231796264648, 6.9301042556762695]\n",
      "epoch 52|60; total loss:22.949081420898438\n",
      "last losses: [1.7268238067626953, 14.828529357910156, 10.466588020324707]\n",
      "epoch 52|90; total loss:23.935842514038086\n",
      "last losses: [3.809178352355957, 15.555704116821289, 11.083148956298828]\n",
      "epoch 52|120; total loss:23.54667091369629\n",
      "last losses: [2.1459238529205322, 13.674877166748047, 7.670738220214844]\n",
      "epoch 52|150; total loss:22.7823429107666\n",
      "last losses: [1.335405945777893, 14.72761058807373, 7.915437698364258]\n",
      "epoch 52|180; total loss:23.846418380737305\n",
      "last losses: [2.943449020385742, 12.076171875, 7.225599765777588]\n",
      "epoch 52|210; total loss:24.003202438354492\n",
      "last losses: [2.5925114154815674, 13.936634063720703, 7.561286449432373]\n",
      "epoch 52|240; total loss:23.864120483398438\n",
      "last losses: [3.5932819843292236, 12.907769203186035, 7.798728942871094]\n",
      "epoch 52|270; total loss:23.220335006713867\n",
      "last losses: [1.3855276107788086, 10.98027229309082, 6.060175895690918]\n",
      "epoch 53|0; total loss:0.6078175902366638\n",
      "last losses: [1.7160775661468506, 10.729281425476074, 5.7891693115234375]\n",
      "epoch 53|30; total loss:23.231435775756836\n",
      "last losses: [1.4334875345230103, 13.150289535522461, 7.671217918395996]\n",
      "epoch 53|60; total loss:22.905122756958008\n",
      "last losses: [1.998871922492981, 10.068735122680664, 7.809141159057617]\n",
      "epoch 53|90; total loss:22.64876365661621\n",
      "last losses: [2.3731908798217773, 12.515573501586914, 7.077671051025391]\n",
      "epoch 53|120; total loss:22.43747901916504\n",
      "last losses: [1.8739161491394043, 13.68925666809082, 9.175968170166016]\n",
      "epoch 53|150; total loss:22.42059898376465\n",
      "last losses: [2.8896679878234863, 12.874505996704102, 8.514909744262695]\n",
      "epoch 53|180; total loss:23.064477920532227\n",
      "last losses: [3.514913558959961, 12.560565948486328, 8.650026321411133]\n",
      "epoch 53|210; total loss:24.54288673400879\n",
      "last losses: [3.445044994354248, 16.956741333007812, 9.656994819641113]\n",
      "epoch 53|240; total loss:22.988101959228516\n",
      "last losses: [2.2908923625946045, 13.061233520507812, 7.920267581939697]\n",
      "epoch 53|270; total loss:24.490276336669922\n",
      "last losses: [2.277068853378296, 14.590566635131836, 7.947273254394531]\n",
      "epoch 54|0; total loss:0.7499138116836548\n",
      "last losses: [3.2901711463928223, 11.790153503417969, 7.417088031768799]\n",
      "epoch 54|30; total loss:23.91869354248047\n",
      "last losses: [1.5434305667877197, 10.770133972167969, 6.861905097961426]\n",
      "epoch 54|60; total loss:21.778709411621094\n",
      "last losses: [1.6852930784225464, 8.481014251708984, 5.079482078552246]\n",
      "epoch 54|90; total loss:22.03632926940918\n",
      "last losses: [3.255166530609131, 15.345969200134277, 8.09103775024414]\n",
      "epoch 54|120; total loss:21.71893882751465\n",
      "last losses: [3.4465222358703613, 15.403397560119629, 11.408889770507812]\n",
      "epoch 54|150; total loss:22.557437896728516\n",
      "last losses: [2.5454704761505127, 10.830646514892578, 7.248544216156006]\n",
      "epoch 54|180; total loss:23.102787017822266\n",
      "last losses: [1.2597283124923706, 10.54071044921875, 5.294445991516113]\n",
      "epoch 54|210; total loss:22.963563919067383\n",
      "last losses: [2.9457621574401855, 11.872539520263672, 9.099961280822754]\n",
      "epoch 54|240; total loss:23.70195198059082\n",
      "last losses: [3.7414932250976562, 15.082094192504883, 8.855578422546387]\n",
      "epoch 54|270; total loss:24.610885620117188\n",
      "last losses: [2.854887008666992, 12.450401306152344, 7.0250654220581055]\n",
      "epoch 55|0; total loss:0.7812916040420532\n",
      "last losses: [2.7583367824554443, 12.460432052612305, 8.219978332519531]\n",
      "epoch 55|30; total loss:23.145761489868164\n",
      "last losses: [2.731301784515381, 12.162246704101562, 8.127943992614746]\n",
      "epoch 55|60; total loss:22.7054500579834\n",
      "last losses: [2.1373491287231445, 13.990507125854492, 9.376890182495117]\n",
      "epoch 55|90; total loss:22.679616928100586\n",
      "last losses: [1.3367174863815308, 11.090520858764648, 7.249485969543457]\n",
      "epoch 55|120; total loss:21.077638626098633\n",
      "last losses: [0.5961048007011414, 10.380941390991211, 5.450712203979492]\n",
      "epoch 55|150; total loss:22.2087459564209\n",
      "last losses: [1.674576759338379, 14.672954559326172, 8.681159973144531]\n",
      "epoch 55|180; total loss:21.833784103393555\n",
      "last losses: [2.531202554702759, 15.157615661621094, 9.996102333068848]\n",
      "epoch 55|210; total loss:22.650259017944336\n",
      "last losses: [4.68392276763916, 11.77233600616455, 7.344142913818359]\n",
      "epoch 55|240; total loss:22.873342514038086\n",
      "last losses: [1.4064005613327026, 11.773603439331055, 7.281681537628174]\n",
      "epoch 55|270; total loss:22.863662719726562\n",
      "last losses: [2.1102728843688965, 12.729815483093262, 7.627852916717529]\n",
      "epoch 56|0; total loss:0.6640670895576477\n",
      "last losses: [2.880976676940918, 11.317822456359863, 5.723214626312256]\n",
      "epoch 56|30; total loss:22.428857803344727\n",
      "last losses: [1.348456859588623, 9.538866996765137, 6.324395179748535]\n",
      "epoch 56|60; total loss:21.487091064453125\n",
      "last losses: [1.6408369541168213, 11.757394790649414, 8.906624794006348]\n",
      "epoch 56|90; total loss:22.024818420410156\n",
      "last losses: [2.3080363273620605, 13.325252532958984, 6.881941795349121]\n",
      "epoch 56|120; total loss:21.89290428161621\n",
      "last losses: [2.1539418697357178, 15.002859115600586, 9.312477111816406]\n",
      "epoch 56|150; total loss:21.286415100097656\n",
      "last losses: [1.6820863485336304, 15.872928619384766, 9.616436004638672]\n",
      "epoch 56|180; total loss:21.328449249267578\n",
      "last losses: [2.1127514839172363, 14.063682556152344, 7.98994255065918]\n",
      "epoch 56|210; total loss:22.443073272705078\n",
      "last losses: [2.5293097496032715, 13.888677597045898, 7.154693603515625]\n",
      "epoch 56|240; total loss:22.18967056274414\n",
      "last losses: [1.3515645265579224, 11.747230529785156, 6.81549072265625]\n",
      "epoch 56|270; total loss:23.00591278076172\n",
      "last losses: [1.713066577911377, 13.165172576904297, 7.394168853759766]\n",
      "epoch 57|0; total loss:0.6806862354278564\n",
      "last losses: [1.8836771249771118, 11.56042766571045, 6.9764814376831055]\n",
      "epoch 57|30; total loss:22.752674102783203\n",
      "last losses: [1.3405745029449463, 10.70610523223877, 6.432623863220215]\n",
      "epoch 57|60; total loss:22.28413200378418\n",
      "last losses: [2.0515737533569336, 10.172411918640137, 5.877418518066406]\n",
      "epoch 57|90; total loss:21.966712951660156\n",
      "last losses: [1.2931703329086304, 11.022225379943848, 6.225014686584473]\n",
      "epoch 57|120; total loss:20.849817276000977\n",
      "last losses: [1.179503083229065, 13.707334518432617, 8.168516159057617]\n",
      "epoch 57|150; total loss:21.560150146484375\n",
      "last losses: [2.1461822986602783, 14.10274600982666, 8.749666213989258]\n",
      "epoch 57|180; total loss:21.14459228515625\n",
      "last losses: [3.3693249225616455, 10.89664077758789, 6.297292709350586]\n",
      "epoch 57|210; total loss:22.05238914489746\n",
      "last losses: [2.547483205795288, 14.910186767578125, 9.835434913635254]\n",
      "epoch 57|240; total loss:22.073463439941406\n",
      "last losses: [2.172785520553589, 11.979313850402832, 7.732813835144043]\n",
      "epoch 57|270; total loss:22.03575325012207\n",
      "last losses: [1.0769762992858887, 8.908333778381348, 5.298974990844727]\n",
      "epoch 58|0; total loss:0.6026998162269592\n",
      "last losses: [1.5936397314071655, 9.989230155944824, 6.498125076293945]\n",
      "epoch 58|30; total loss:21.35922622680664\n",
      "last losses: [0.7497439384460449, 11.104190826416016, 6.557747840881348]\n",
      "epoch 58|60; total loss:21.012775421142578\n",
      "last losses: [2.7236289978027344, 10.180427551269531, 5.404354095458984]\n",
      "epoch 58|90; total loss:21.608654022216797\n",
      "last losses: [1.472733736038208, 10.707182884216309, 5.545507431030273]\n",
      "epoch 58|120; total loss:20.74106788635254\n",
      "last losses: [1.2244584560394287, 13.610729217529297, 8.562504768371582]\n",
      "epoch 58|150; total loss:20.678335189819336\n",
      "last losses: [0.4504505693912506, 8.931665420532227, 7.063097953796387]\n",
      "epoch 58|180; total loss:20.154006958007812\n",
      "last losses: [1.176203966140747, 10.343633651733398, 6.615442752838135]\n",
      "epoch 58|210; total loss:20.739519119262695\n",
      "last losses: [1.4610226154327393, 12.671544075012207, 7.739696502685547]\n",
      "epoch 58|240; total loss:21.654104232788086\n",
      "last losses: [1.4094363451004028, 11.058736801147461, 7.570197582244873]\n",
      "epoch 58|270; total loss:21.746540069580078\n",
      "last losses: [1.6995298862457275, 10.271854400634766, 7.472177028656006]\n",
      "epoch 59|0; total loss:0.9735127091407776\n",
      "last losses: [2.4715633392333984, 15.82861328125, 10.905203819274902]\n",
      "epoch 59|30; total loss:20.706998825073242\n",
      "last losses: [1.0816271305084229, 11.235918045043945, 7.364341735839844]\n",
      "epoch 59|60; total loss:21.422252655029297\n",
      "last losses: [2.757951498031616, 10.323646545410156, 7.113527297973633]\n",
      "epoch 59|90; total loss:19.772052764892578\n",
      "last losses: [0.9019079208374023, 10.356761932373047, 6.896468162536621]\n",
      "epoch 59|120; total loss:21.760164260864258\n",
      "last losses: [3.9558653831481934, 12.02519416809082, 7.253767967224121]\n",
      "epoch 59|150; total loss:19.90011215209961\n",
      "last losses: [0.8299765586853027, 10.288017272949219, 6.227043628692627]\n",
      "epoch 59|180; total loss:20.667007446289062\n",
      "last losses: [0.609056293964386, 12.265872955322266, 7.510261058807373]\n",
      "epoch 59|210; total loss:21.263137817382812\n",
      "last losses: [1.250573992729187, 12.119848251342773, 5.6683244705200195]\n",
      "epoch 59|240; total loss:20.133407592773438\n",
      "last losses: [1.0295557975769043, 11.736580848693848, 7.910334587097168]\n",
      "epoch 59|270; total loss:20.45244026184082\n",
      "last losses: [1.2521047592163086, 9.220356941223145, 5.084515571594238]\n",
      "epoch 60|0; total loss:0.7868838310241699\n",
      "last losses: [1.6670541763305664, 13.508484840393066, 8.430975914001465]\n",
      "epoch 60|30; total loss:21.405418395996094\n",
      "last losses: [3.5006446838378906, 13.983846664428711, 8.367740631103516]\n",
      "epoch 60|60; total loss:20.391183853149414\n",
      "last losses: [0.8762576580047607, 10.675580978393555, 6.424211025238037]\n",
      "epoch 60|90; total loss:20.325037002563477\n",
      "last losses: [1.0481774806976318, 12.605598449707031, 5.958151817321777]\n",
      "epoch 60|120; total loss:20.39203453063965\n",
      "last losses: [1.311903476715088, 11.590200424194336, 7.142021179199219]\n",
      "epoch 60|150; total loss:20.648086547851562\n",
      "last losses: [1.0110132694244385, 10.358741760253906, 7.905435562133789]\n",
      "epoch 60|180; total loss:21.110790252685547\n",
      "last losses: [1.9267120361328125, 13.273591995239258, 8.897787094116211]\n",
      "epoch 60|210; total loss:20.893678665161133\n",
      "last losses: [1.7317968606948853, 10.04367733001709, 7.411143779754639]\n",
      "epoch 60|240; total loss:21.162738800048828\n",
      "last losses: [1.3015153408050537, 10.281896591186523, 7.279112815856934]\n",
      "epoch 60|270; total loss:19.622512817382812\n",
      "last losses: [1.1514456272125244, 8.292378425598145, 6.984675884246826]\n",
      "epoch 61|0; total loss:0.5909826755523682\n",
      "last losses: [1.1603443622589111, 10.156124114990234, 6.413010597229004]\n",
      "epoch 61|30; total loss:20.905778884887695\n",
      "last losses: [1.111907958984375, 9.404779434204102, 5.493442058563232]\n",
      "epoch 61|60; total loss:20.3493595123291\n",
      "last losses: [0.8918592929840088, 7.95565938949585, 5.631051063537598]\n",
      "epoch 61|90; total loss:18.99382972717285\n",
      "last losses: [1.2174241542816162, 7.757570743560791, 6.465823173522949]\n",
      "epoch 61|120; total loss:20.335330963134766\n",
      "last losses: [1.2748069763183594, 13.06232738494873, 8.968343734741211]\n",
      "epoch 61|150; total loss:20.484968185424805\n",
      "last losses: [3.9796242713928223, 12.865158081054688, 8.525399208068848]\n",
      "epoch 61|180; total loss:21.466997146606445\n",
      "last losses: [1.4335277080535889, 10.24026870727539, 7.4923834800720215]\n",
      "epoch 61|210; total loss:20.413869857788086\n",
      "last losses: [3.2688541412353516, 10.005523681640625, 5.760652542114258]\n",
      "epoch 61|240; total loss:19.83209800720215\n",
      "last losses: [1.0566126108169556, 11.923202514648438, 6.015564918518066]\n",
      "epoch 61|270; total loss:20.92225456237793\n",
      "last losses: [4.255244731903076, 13.2186861038208, 7.809518337249756]\n",
      "epoch 62|0; total loss:0.5986289381980896\n",
      "last losses: [0.9281396865844727, 10.237780570983887, 6.792947292327881]\n",
      "epoch 62|30; total loss:19.598976135253906\n",
      "last losses: [1.6507418155670166, 9.460681915283203, 7.16560173034668]\n",
      "epoch 62|60; total loss:19.20400047302246\n",
      "last losses: [1.317387342453003, 10.729496955871582, 6.986849308013916]\n",
      "epoch 62|90; total loss:19.353477478027344\n",
      "last losses: [1.5083787441253662, 11.154003143310547, 7.15205192565918]\n",
      "epoch 62|120; total loss:18.68328094482422\n",
      "last losses: [3.7541027069091797, 15.17985725402832, 9.435840606689453]\n",
      "epoch 62|150; total loss:18.815994262695312\n",
      "last losses: [0.7763700485229492, 10.104914665222168, 7.534775733947754]\n",
      "epoch 62|180; total loss:19.771268844604492\n",
      "last losses: [3.440539598464966, 8.125738143920898, 4.952773094177246]\n",
      "epoch 62|210; total loss:19.612550735473633\n",
      "last losses: [1.6480029821395874, 10.934150695800781, 6.822698593139648]\n",
      "epoch 62|240; total loss:19.37779426574707\n",
      "last losses: [3.082984447479248, 12.008100509643555, 8.007474899291992]\n",
      "epoch 62|270; total loss:20.341787338256836\n",
      "last losses: [1.4230751991271973, 13.523250579833984, 7.966150283813477]\n",
      "epoch 63|0; total loss:0.599819540977478\n",
      "last losses: [0.8634814023971558, 10.389530181884766, 6.741573810577393]\n",
      "epoch 63|30; total loss:19.01034164428711\n",
      "last losses: [0.9113107919692993, 13.240068435668945, 9.739738464355469]\n",
      "epoch 63|60; total loss:19.42231559753418\n",
      "last losses: [1.474934697151184, 10.501739501953125, 7.527657508850098]\n",
      "epoch 63|90; total loss:19.15131378173828\n",
      "last losses: [1.354337215423584, 11.083703994750977, 7.593358993530273]\n",
      "epoch 63|120; total loss:19.790786743164062\n",
      "last losses: [0.489663690328598, 12.531983375549316, 8.825210571289062]\n",
      "epoch 63|150; total loss:18.655061721801758\n",
      "last losses: [1.398406982421875, 11.364798545837402, 8.456670761108398]\n",
      "epoch 63|180; total loss:19.683046340942383\n",
      "last losses: [2.832474708557129, 13.356098175048828, 8.412439346313477]\n",
      "epoch 63|210; total loss:19.122730255126953\n",
      "last losses: [2.543870687484741, 9.756515502929688, 6.168543815612793]\n",
      "epoch 63|240; total loss:19.95757293701172\n",
      "last losses: [3.40161395072937, 11.533766746520996, 8.22497844696045]\n",
      "epoch 63|270; total loss:18.989273071289062\n",
      "last losses: [1.656996488571167, 11.501334190368652, 6.157960414886475]\n",
      "epoch 64|0; total loss:0.7728065848350525\n",
      "last losses: [1.3279907703399658, 13.338279724121094, 8.517927169799805]\n",
      "epoch 64|30; total loss:19.895584106445312\n",
      "last losses: [0.8855482935905457, 8.188157081604004, 5.289804458618164]\n",
      "epoch 64|60; total loss:20.394275665283203\n",
      "last losses: [0.8419671058654785, 7.496575832366943, 5.296265602111816]\n",
      "epoch 64|90; total loss:19.836397171020508\n",
      "last losses: [1.130663514137268, 9.547981262207031, 6.133769989013672]\n",
      "epoch 64|120; total loss:18.89748764038086\n",
      "last losses: [1.2764383554458618, 10.803812980651855, 7.590106010437012]\n",
      "epoch 64|150; total loss:18.774930953979492\n",
      "last losses: [1.7095743417739868, 12.347679138183594, 8.52853012084961]\n",
      "epoch 64|180; total loss:19.316022872924805\n",
      "last losses: [2.9944541454315186, 9.688713073730469, 5.660539627075195]\n",
      "epoch 64|210; total loss:19.681964874267578\n",
      "last losses: [1.4736549854278564, 9.456367492675781, 8.213205337524414]\n",
      "epoch 64|240; total loss:20.58835220336914\n",
      "last losses: [1.3299050331115723, 11.391275405883789, 9.123008728027344]\n",
      "epoch 64|270; total loss:20.115764617919922\n",
      "last losses: [2.231060743331909, 7.9565510749816895, 5.873656749725342]\n",
      "epoch 65|0; total loss:0.6054338216781616\n",
      "last losses: [1.3421279191970825, 10.342544555664062, 6.478340148925781]\n",
      "epoch 65|30; total loss:19.506729125976562\n",
      "last losses: [0.704317033290863, 10.961244583129883, 7.860490798950195]\n",
      "epoch 65|60; total loss:18.76366424560547\n",
      "last losses: [3.6032323837280273, 9.044084548950195, 6.103240013122559]\n",
      "epoch 65|90; total loss:18.958084106445312\n",
      "last losses: [0.9276747703552246, 10.866975784301758, 7.1226420402526855]\n",
      "epoch 65|120; total loss:19.25126075744629\n",
      "last losses: [1.4040149450302124, 8.112459182739258, 5.998824119567871]\n",
      "epoch 65|150; total loss:18.806074142456055\n",
      "last losses: [1.4697265625, 13.444756507873535, 9.359090805053711]\n",
      "epoch 65|180; total loss:18.307941436767578\n",
      "last losses: [1.688254714012146, 13.162842750549316, 8.90281867980957]\n",
      "epoch 65|210; total loss:19.592018127441406\n",
      "last losses: [3.3738677501678467, 12.900229454040527, 8.691999435424805]\n",
      "epoch 65|240; total loss:20.657913208007812\n",
      "last losses: [1.0522493124008179, 10.421107292175293, 6.136445999145508]\n",
      "epoch 65|270; total loss:22.664995193481445\n",
      "last losses: [3.748852491378784, 10.97260856628418, 7.773864269256592]\n",
      "epoch 66|0; total loss:0.6158920526504517\n",
      "last losses: [0.7975035905838013, 9.761628150939941, 7.917628288269043]\n",
      "epoch 66|30; total loss:19.848434448242188\n",
      "last losses: [1.0279581546783447, 12.438968658447266, 7.510776042938232]\n",
      "epoch 66|60; total loss:19.279979705810547\n",
      "last losses: [3.4164092540740967, 11.731287956237793, 5.822969913482666]\n",
      "epoch 66|90; total loss:19.055370330810547\n",
      "last losses: [1.2742369174957275, 11.337249755859375, 7.991910934448242]\n",
      "epoch 66|120; total loss:20.201845169067383\n",
      "last losses: [1.5368731021881104, 11.687560081481934, 9.505215644836426]\n",
      "epoch 66|150; total loss:19.560894012451172\n",
      "last losses: [1.1269437074661255, 9.487771034240723, 7.45505952835083]\n",
      "epoch 66|180; total loss:18.99919319152832\n",
      "last losses: [2.196693181991577, 9.402910232543945, 5.919468402862549]\n",
      "epoch 66|210; total loss:19.578556060791016\n",
      "last losses: [1.476866602897644, 8.120222091674805, 7.157498359680176]\n",
      "epoch 66|240; total loss:18.81601333618164\n",
      "last losses: [1.3367998600006104, 11.722172737121582, 7.864409923553467]\n",
      "epoch 66|270; total loss:18.837783813476562\n",
      "last losses: [0.8204391002655029, 8.898902893066406, 5.903041839599609]\n",
      "epoch 67|0; total loss:0.626755952835083\n",
      "last losses: [0.7901356220245361, 10.8608980178833, 7.151642322540283]\n",
      "epoch 67|30; total loss:18.813840866088867\n",
      "last losses: [2.6043217182159424, 12.785690307617188, 8.832192420959473]\n",
      "epoch 67|60; total loss:17.653667449951172\n",
      "last losses: [0.4331817328929901, 8.165510177612305, 4.085276126861572]\n",
      "epoch 67|90; total loss:18.645809173583984\n",
      "last losses: [0.45831140875816345, 8.519434928894043, 7.068842887878418]\n",
      "epoch 67|120; total loss:18.589771270751953\n",
      "last losses: [1.076845407485962, 9.260299682617188, 5.794565677642822]\n",
      "epoch 67|150; total loss:18.68160057067871\n",
      "last losses: [1.657464623451233, 8.382585525512695, 7.317706108093262]\n",
      "epoch 67|180; total loss:19.358909606933594\n",
      "last losses: [1.236539363861084, 10.4042329788208, 7.212026596069336]\n",
      "epoch 67|210; total loss:18.79322624206543\n",
      "last losses: [1.6911826133728027, 11.311235427856445, 7.1976118087768555]\n",
      "epoch 67|240; total loss:18.343650817871094\n",
      "last losses: [0.9209619760513306, 8.547584533691406, 6.063368797302246]\n",
      "epoch 67|270; total loss:18.50615119934082\n",
      "last losses: [1.2819795608520508, 11.150522232055664, 7.164058208465576]\n",
      "epoch 68|0; total loss:0.7380307912826538\n",
      "last losses: [1.941420555114746, 12.321369171142578, 7.8781328201293945]\n",
      "epoch 68|30; total loss:18.6577205657959\n",
      "last losses: [0.720505952835083, 9.505964279174805, 6.599933624267578]\n",
      "epoch 68|60; total loss:17.140491485595703\n",
      "last losses: [3.2048990726470947, 8.481546401977539, 5.046012878417969]\n",
      "epoch 68|90; total loss:18.360347747802734\n",
      "last losses: [0.9479761719703674, 10.0128812789917, 5.943861961364746]\n",
      "epoch 68|120; total loss:17.903018951416016\n",
      "last losses: [0.7351516485214233, 10.027795791625977, 7.818042755126953]\n",
      "epoch 68|150; total loss:17.49107551574707\n",
      "last losses: [0.892939567565918, 7.011714935302734, 6.138425350189209]\n",
      "epoch 68|180; total loss:18.780805587768555\n",
      "last losses: [0.7759902477264404, 7.189121246337891, 5.456913948059082]\n",
      "epoch 68|210; total loss:17.76659393310547\n",
      "last losses: [1.1104313135147095, 10.935826301574707, 6.659344673156738]\n",
      "epoch 68|240; total loss:17.568988800048828\n",
      "last losses: [1.3095042705535889, 8.06184196472168, 6.306550979614258]\n",
      "epoch 68|270; total loss:18.01434898376465\n",
      "last losses: [1.3486130237579346, 13.379611015319824, 9.863876342773438]\n",
      "epoch 69|0; total loss:0.4332696497440338\n",
      "last losses: [0.582434356212616, 7.3492431640625, 5.06641149520874]\n",
      "epoch 69|30; total loss:18.164608001708984\n",
      "last losses: [0.852217435836792, 9.464608192443848, 6.78804349899292]\n",
      "epoch 69|60; total loss:17.416770935058594\n",
      "last losses: [1.3595271110534668, 11.172247886657715, 7.667566299438477]\n",
      "epoch 69|90; total loss:18.251644134521484\n",
      "last losses: [0.7274333238601685, 9.4793701171875, 6.1725311279296875]\n",
      "epoch 69|120; total loss:17.82809829711914\n",
      "last losses: [1.3517882823944092, 11.845008850097656, 7.417665958404541]\n",
      "epoch 69|150; total loss:17.720836639404297\n",
      "last losses: [1.0802754163742065, 9.761724472045898, 6.280745506286621]\n",
      "epoch 69|180; total loss:17.809677124023438\n",
      "last losses: [1.289522647857666, 9.932367324829102, 7.125383377075195]\n",
      "epoch 69|210; total loss:17.822004318237305\n",
      "last losses: [0.89105224609375, 7.758595943450928, 7.173184871673584]\n",
      "epoch 69|240; total loss:18.689973831176758\n",
      "last losses: [0.9039539098739624, 9.900951385498047, 6.293646812438965]\n",
      "epoch 69|270; total loss:18.59001350402832\n",
      "last losses: [1.141142725944519, 10.500713348388672, 7.144481182098389]\n",
      "epoch 70|0; total loss:0.42957866191864014\n",
      "last losses: [0.366942822933197, 7.418516159057617, 5.101901054382324]\n",
      "epoch 70|30; total loss:17.6608943939209\n",
      "last losses: [1.475696086883545, 10.91318416595459, 6.99654483795166]\n",
      "epoch 70|60; total loss:18.02324676513672\n",
      "last losses: [1.571842908859253, 10.895275115966797, 6.661556720733643]\n",
      "epoch 70|90; total loss:17.954233169555664\n",
      "last losses: [0.9887897968292236, 11.403478622436523, 7.075445652008057]\n",
      "epoch 70|120; total loss:16.970422744750977\n",
      "last losses: [5.025979042053223, 8.595935821533203, 5.021188735961914]\n",
      "epoch 70|150; total loss:16.973018646240234\n",
      "last losses: [0.6986706852912903, 8.626930236816406, 6.519536018371582]\n",
      "epoch 70|180; total loss:18.093914031982422\n",
      "last losses: [0.6004883050918579, 8.813718795776367, 5.554001808166504]\n",
      "epoch 70|210; total loss:17.50316047668457\n",
      "last losses: [0.3233717679977417, 9.372933387756348, 7.327502727508545]\n",
      "epoch 70|240; total loss:18.42530632019043\n",
      "last losses: [1.795032024383545, 11.770733833312988, 6.86403751373291]\n",
      "epoch 70|270; total loss:18.639938354492188\n",
      "last losses: [0.9593554735183716, 10.031998634338379, 7.05226993560791]\n",
      "epoch 71|0; total loss:0.6278067827224731\n",
      "last losses: [1.786668062210083, 10.624330520629883, 6.4232025146484375]\n",
      "epoch 71|30; total loss:18.91946792602539\n",
      "last losses: [2.371321678161621, 10.202990531921387, 6.251501083374023]\n",
      "epoch 71|60; total loss:17.997758865356445\n",
      "last losses: [1.180167317390442, 8.436467170715332, 5.229524612426758]\n",
      "epoch 71|90; total loss:18.155319213867188\n",
      "last losses: [1.1994149684906006, 12.71414566040039, 7.566972732543945]\n",
      "epoch 71|120; total loss:18.70004653930664\n",
      "last losses: [2.0016977787017822, 12.430811882019043, 7.5740065574646]\n",
      "epoch 71|150; total loss:18.08678436279297\n",
      "last losses: [1.0781326293945312, 7.791977405548096, 5.405517101287842]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexander/computerScience/phystech/9sem/abbyy/course_cvdl/task2/abbyy_course_cvdl_t2/impl/train.py:64: UserWarning: nan loss! skip update\n",
      "  warnings.warn(\"nan loss! skip update\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last loss: [nan, 93.0347900390625, nan]\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "net = train(ds_train, net=net, criterion=crit, batch_size=64, epochs=120, device=torch.device('cuda:0'), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, 'centernet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.load('centernet.pth').to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 100, 6])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno = torch.load('anno.pt')\n",
    "anno.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 3, 256, 256)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.load('inputs.pt')\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "         ...,\n",
       "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]],\n",
       "\n",
       "        [[  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n",
       "\n",
       "        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         ...,\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],\n",
       "\n",
       "        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         ...,\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf]]],\n",
       "\n",
       "\n",
       "       [[[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "         ...,\n",
       "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]],\n",
       "\n",
       "        [[  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n",
       "\n",
       "        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         ...,\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],\n",
       "\n",
       "        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         ...,\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf]]],\n",
       "\n",
       "\n",
       "       [[[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "         ...,\n",
       "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]],\n",
       "\n",
       "        [[  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n",
       "\n",
       "        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         ...,\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],\n",
       "\n",
       "        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         ...,\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "         ...,\n",
       "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]],\n",
       "\n",
       "        [[  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n",
       "\n",
       "        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         ...,\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],\n",
       "\n",
       "        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         ...,\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf]]],\n",
       "\n",
       "\n",
       "       [[[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "         ...,\n",
       "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]],\n",
       "\n",
       "        [[  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n",
       "\n",
       "        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         ...,\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],\n",
       "\n",
       "        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         ...,\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf]]],\n",
       "\n",
       "\n",
       "       [[[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "         ...,\n",
       "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]],\n",
       "\n",
       "        [[  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n",
       "\n",
       "        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         ...,\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],\n",
       "\n",
       "        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         ...,\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, ..., -inf, -inf, -inf]]]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = torch.load('outputs.pt')\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          ...,\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan]],\n",
      "\n",
      "         [[        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          ...,\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan]],\n",
      "\n",
      "         [[        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          ...,\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan]]],\n",
      "\n",
      "\n",
      "        [[[        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          ...,\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan]],\n",
      "\n",
      "         [[        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          ...,\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan]],\n",
      "\n",
      "         [[        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          ...,\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan]]],\n",
      "\n",
      "\n",
      "        [[[-7.0826e-08, -6.4306e-08, -7.3806e-08,  ..., -9.8000e-08,\n",
      "           -1.0905e-07, -8.3421e-08],\n",
      "          [-6.1125e-09,  2.0613e-09, -8.0922e-09,  ..., -4.9840e-08,\n",
      "           -4.3836e-08, -3.0538e-09],\n",
      "          [ 7.1953e-08,  7.5616e-08,  5.9282e-08,  ..., -9.7509e-09,\n",
      "           -1.0951e-09,  4.2442e-08],\n",
      "          ...,\n",
      "          [ 9.5889e-08,  1.0039e-07,  7.9817e-08,  ..., -1.7491e-08,\n",
      "           -4.7666e-08, -1.3265e-08],\n",
      "          [ 1.2904e-07,  1.4762e-07,  1.7477e-07,  ...,  1.3233e-07,\n",
      "            1.0628e-07,  9.3316e-08],\n",
      "          [ 1.2558e-07,  1.3644e-07,  1.8431e-07,  ...,  2.1399e-07,\n",
      "            1.7710e-07,  1.7166e-07]],\n",
      "\n",
      "         [[-1.2690e-07, -9.6139e-08, -1.0372e-07,  ..., -1.1808e-07,\n",
      "           -1.3309e-07, -1.0820e-07],\n",
      "          [-5.7412e-08, -2.5055e-08, -3.0115e-08,  ..., -7.2922e-08,\n",
      "           -6.7022e-08, -2.2574e-08],\n",
      "          [ 2.1813e-08,  4.8608e-08,  3.1222e-08,  ..., -1.8694e-08,\n",
      "           -7.9591e-09,  3.9750e-08],\n",
      "          ...,\n",
      "          [ 5.6013e-08,  7.5526e-08,  4.4496e-08,  ..., -4.4128e-08,\n",
      "           -5.9930e-08, -1.8247e-08],\n",
      "          [ 7.7614e-08,  9.8348e-08,  1.0455e-07,  ...,  6.3272e-08,\n",
      "            4.1781e-08,  4.5901e-08],\n",
      "          [ 5.9834e-08,  7.1006e-08,  9.0437e-08,  ...,  1.1654e-07,\n",
      "            8.7550e-08,  9.8837e-08]],\n",
      "\n",
      "         [[-4.3810e-08,  1.3270e-08,  7.8275e-09,  ..., -5.8804e-09,\n",
      "           -2.6217e-08, -1.5649e-08],\n",
      "          [ 4.1700e-08,  1.0778e-07,  1.0946e-07,  ...,  7.6403e-08,\n",
      "            7.1450e-08,  9.7615e-08],\n",
      "          [ 1.0436e-07,  1.6586e-07,  1.5933e-07,  ...,  1.3517e-07,\n",
      "            1.3487e-07,  1.6449e-07],\n",
      "          ...,\n",
      "          [ 9.8763e-08,  1.5072e-07,  1.2547e-07,  ...,  6.8316e-08,\n",
      "            6.8382e-08,  1.1367e-07],\n",
      "          [ 9.1435e-08,  1.3576e-07,  1.3793e-07,  ...,  1.1678e-07,\n",
      "            1.1723e-07,  1.4394e-07],\n",
      "          [ 6.2183e-08,  8.8184e-08,  1.0456e-07,  ...,  1.3941e-07,\n",
      "            1.3333e-07,  1.5844e-07]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          ...,\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan]],\n",
      "\n",
      "         [[        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          ...,\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan]],\n",
      "\n",
      "         [[        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          ...,\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan]]],\n",
      "\n",
      "\n",
      "        [[[        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          ...,\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan]],\n",
      "\n",
      "         [[        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          ...,\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan]],\n",
      "\n",
      "         [[        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          ...,\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan]]],\n",
      "\n",
      "\n",
      "        [[[        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          ...,\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan]],\n",
      "\n",
      "         [[        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          ...,\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan]],\n",
      "\n",
      "         [[        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          ...,\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan]]]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([        nan,         nan, -5.1096e-08,         nan,  3.4404e-09,\n",
      "                nan,         nan,  1.3153e-07,         nan,  1.5152e-06,\n",
      "                nan,         nan,         nan,  1.0862e-05,         nan,\n",
      "                nan,         nan,         nan,         nan,         nan,\n",
      "                nan,         nan,         nan,         nan,         nan,\n",
      "                nan,         nan,         nan,         nan,         nan,\n",
      "                nan,         nan,         nan,         nan,         nan,\n",
      "                nan,  7.4804e-08,         nan,  1.4740e-08,         nan,\n",
      "                nan,         nan,         nan,         nan,         nan,\n",
      "                nan,         nan,         nan,  2.2001e-08,         nan,\n",
      "                nan,         nan,         nan,         nan,  2.4635e-01,\n",
      "                nan,         nan,         nan,         nan,         nan,\n",
      "                nan,         nan,         nan,         nan], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([        nan,         nan, -1.0543e-06,         nan, -1.6571e-08,\n",
      "                nan,         nan, -4.3020e-07,         nan, -8.0052e-06,\n",
      "                nan,         nan,         nan, -3.4749e-05,         nan,\n",
      "                nan,         nan,         nan,         nan,         nan,\n",
      "                nan,         nan,         nan,         nan,         nan,\n",
      "                nan,         nan,         nan,         nan,         nan,\n",
      "                nan,         nan,         nan,         nan,         nan,\n",
      "                nan, -2.4507e-07,         nan, -4.9152e-08,         nan,\n",
      "                nan,         nan,         nan,         nan,         nan,\n",
      "                nan,         nan,         nan, -8.6909e-08,         nan,\n",
      "                nan,         nan,         nan,         nan, -5.1959e-01,\n",
      "                nan,         nan,         nan,         nan,         nan,\n",
      "                nan,         nan,         nan,         nan], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]]],\n",
      "\n",
      "\n",
      "        [[[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]]],\n",
      "\n",
      "\n",
      "        [[[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]]],\n",
      "\n",
      "\n",
      "        [[[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]]],\n",
      "\n",
      "\n",
      "        [[[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]]]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]]],\n",
      "\n",
      "\n",
      "        [[[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]]],\n",
      "\n",
      "\n",
      "        [[[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]]],\n",
      "\n",
      "\n",
      "        [[[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]]],\n",
      "\n",
      "\n",
      "        [[[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]]]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]]],\n",
      "\n",
      "\n",
      "        [[[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]]],\n",
      "\n",
      "\n",
      "        [[[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]]],\n",
      "\n",
      "\n",
      "        [[[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]]],\n",
      "\n",
      "\n",
      "        [[[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]]]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan, 8.3289,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan, 1.0447,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
      "            nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
      "            nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
      "            nan,     nan,     nan,     nan,     nan,     nan, -0.2238,     nan,\n",
      "            nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
      "            nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
      "            nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
      "            nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[-0.0194,  0.0241, -0.0425],\n",
      "          [-0.0500,  0.0123, -0.0421],\n",
      "          [-0.0373,  0.0305,  0.0273]],\n",
      "\n",
      "         [[ 0.0190, -0.0377,  0.0148],\n",
      "          [-0.0373, -0.0074,  0.0282],\n",
      "          [ 0.0129,  0.0227,  0.0166]],\n",
      "\n",
      "         [[ 0.0095,  0.0219,  0.0067],\n",
      "          [-0.0210, -0.0149,  0.0139],\n",
      "          [ 0.0115, -0.0127, -0.0138]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0226,  0.0187, -0.0207],\n",
      "          [-0.0389, -0.0442, -0.0326],\n",
      "          [ 0.0286, -0.0243, -0.0305]],\n",
      "\n",
      "         [[-0.0439, -0.0172, -0.0114],\n",
      "          [-0.0319, -0.0298, -0.0020],\n",
      "          [-0.0128, -0.0277, -0.0431]],\n",
      "\n",
      "         [[-0.0108, -0.0202, -0.0331],\n",
      "          [-0.0341, -0.0232, -0.0384],\n",
      "          [ 0.0075,  0.0013, -0.0042]]],\n",
      "\n",
      "\n",
      "        [[[    nan,     nan,     nan],\n",
      "          [    nan,     nan,     nan],\n",
      "          [    nan,     nan,     nan]],\n",
      "\n",
      "         [[    nan,     nan,     nan],\n",
      "          [    nan,     nan,     nan],\n",
      "          [    nan,     nan,     nan]],\n",
      "\n",
      "         [[    nan,     nan,     nan],\n",
      "          [    nan,     nan,     nan],\n",
      "          [    nan,     nan,     nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[    nan,     nan,     nan],\n",
      "          [    nan,     nan,     nan],\n",
      "          [    nan,     nan,     nan]],\n",
      "\n",
      "         [[    nan,     nan,     nan],\n",
      "          [    nan,     nan,     nan],\n",
      "          [    nan,     nan,     nan]],\n",
      "\n",
      "         [[    nan,     nan,     nan],\n",
      "          [    nan,     nan,     nan],\n",
      "          [    nan,     nan,     nan]]],\n",
      "\n",
      "\n",
      "        [[[    nan,     nan,     nan],\n",
      "          [    nan,     nan,     nan],\n",
      "          [    nan,     nan,     nan]],\n",
      "\n",
      "         [[    nan,     nan,     nan],\n",
      "          [    nan,     nan,     nan],\n",
      "          [    nan,     nan,     nan]],\n",
      "\n",
      "         [[    nan,     nan,     nan],\n",
      "          [    nan,     nan,     nan],\n",
      "          [    nan,     nan,     nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[    nan,     nan,     nan],\n",
      "          [    nan,     nan,     nan],\n",
      "          [    nan,     nan,     nan]],\n",
      "\n",
      "         [[    nan,     nan,     nan],\n",
      "          [    nan,     nan,     nan],\n",
      "          [    nan,     nan,     nan]],\n",
      "\n",
      "         [[    nan,     nan,     nan],\n",
      "          [    nan,     nan,     nan],\n",
      "          [    nan,     nan,     nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[    nan,     nan,     nan],\n",
      "          [    nan,     nan,     nan],\n",
      "          [    nan,     nan,     nan]],\n",
      "\n",
      "         [[    nan,     nan,     nan],\n",
      "          [    nan,     nan,     nan],\n",
      "          [    nan,     nan,     nan]],\n",
      "\n",
      "         [[    nan,     nan,     nan],\n",
      "          [    nan,     nan,     nan],\n",
      "          [    nan,     nan,     nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[    nan,     nan,     nan],\n",
      "          [    nan,     nan,     nan],\n",
      "          [    nan,     nan,     nan]],\n",
      "\n",
      "         [[    nan,     nan,     nan],\n",
      "          [    nan,     nan,     nan],\n",
      "          [    nan,     nan,     nan]],\n",
      "\n",
      "         [[    nan,     nan,     nan],\n",
      "          [    nan,     nan,     nan],\n",
      "          [    nan,     nan,     nan]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0332, -0.0223, -0.0267],\n",
      "          [-0.1065, -0.0179, -0.0566],\n",
      "          [ 0.0033, -0.1513, -0.0104]],\n",
      "\n",
      "         [[-0.0662,  0.0227, -0.0162],\n",
      "          [-0.0596, -0.0079, -0.0039],\n",
      "          [-0.0054,  0.0473, -0.0290]],\n",
      "\n",
      "         [[-0.0264, -0.0296, -0.0969],\n",
      "          [-0.0347, -0.0844, -0.0707],\n",
      "          [ 0.0127, -0.1411, -0.0372]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0868,  0.0724, -0.0179],\n",
      "          [ 0.0632, -0.0071, -0.0203],\n",
      "          [-0.1425, -0.0094, -0.0217]],\n",
      "\n",
      "         [[-0.0581, -0.0097, -0.0401],\n",
      "          [-0.0397, -0.0759, -0.1026],\n",
      "          [-0.0353, -0.1099, -0.1343]],\n",
      "\n",
      "         [[ 0.0408,  0.0159, -0.0093],\n",
      "          [-0.0360, -0.0138,  0.0072],\n",
      "          [ 0.0088, -0.0018,  0.0448]]],\n",
      "\n",
      "\n",
      "        [[[-0.0343, -0.0152, -0.0251],\n",
      "          [ 0.0234,  0.0271,  0.0002],\n",
      "          [-0.0226,  0.0290, -0.0323]],\n",
      "\n",
      "         [[ 0.0005,  0.0303, -0.0240],\n",
      "          [-0.0383, -0.0375, -0.0308],\n",
      "          [ 0.0221, -0.0029, -0.0263]],\n",
      "\n",
      "         [[-0.0434, -0.0187, -0.0304],\n",
      "          [-0.0214, -0.0410,  0.0028],\n",
      "          [-0.0270,  0.0031, -0.0458]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0302, -0.0176,  0.0111],\n",
      "          [ 0.0040,  0.0100, -0.0089],\n",
      "          [ 0.0319,  0.0218, -0.0213]],\n",
      "\n",
      "         [[-0.0140,  0.0113,  0.0048],\n",
      "          [-0.0067,  0.0033, -0.0337],\n",
      "          [ 0.0150, -0.0236,  0.0140]],\n",
      "\n",
      "         [[-0.0280, -0.0482,  0.0154],\n",
      "          [-0.0344, -0.0062, -0.0407],\n",
      "          [-0.0083,  0.0241,  0.0203]]]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0174,     nan,     nan, -0.0074, -0.0212,     nan, -0.0457, -0.0235,\n",
      "            nan,     nan,     nan,  0.0047,     nan,     nan,     nan, -0.0042,\n",
      "         0.0125, -0.0513,     nan,     nan,     nan,     nan,     nan,     nan,\n",
      "        -0.0366,     nan,     nan, -0.0222,     nan,     nan, -0.0145,     nan,\n",
      "            nan,     nan,  0.0443, -0.0232,  0.0249,     nan, -0.0474, -0.0449,\n",
      "            nan,     nan, -0.0172,  0.0214, -0.0044,     nan,     nan,  0.0298,\n",
      "            nan, -0.0235,  0.0928,     nan,     nan,  0.0149,     nan,  0.0115,\n",
      "        -0.0451,     nan,     nan,  0.0280,  0.0471,     nan,  0.0501, -0.0269],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]],\n",
      "\n",
      "         [[    nan]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1180]],\n",
      "\n",
      "         [[-0.3047]],\n",
      "\n",
      "         [[-0.0168]],\n",
      "\n",
      "         [[-0.1253]],\n",
      "\n",
      "         [[ 0.1787]],\n",
      "\n",
      "         [[ 0.0019]],\n",
      "\n",
      "         [[-0.0846]],\n",
      "\n",
      "         [[-0.0054]],\n",
      "\n",
      "         [[ 0.0137]],\n",
      "\n",
      "         [[-0.2322]],\n",
      "\n",
      "         [[-0.1113]],\n",
      "\n",
      "         [[ 0.0543]],\n",
      "\n",
      "         [[-0.0080]],\n",
      "\n",
      "         [[-0.1593]],\n",
      "\n",
      "         [[ 0.1825]],\n",
      "\n",
      "         [[ 0.0057]],\n",
      "\n",
      "         [[ 0.0005]],\n",
      "\n",
      "         [[ 0.0526]],\n",
      "\n",
      "         [[ 0.0881]],\n",
      "\n",
      "         [[ 0.0272]],\n",
      "\n",
      "         [[-0.1055]],\n",
      "\n",
      "         [[-0.0395]],\n",
      "\n",
      "         [[-0.0020]],\n",
      "\n",
      "         [[-0.3474]],\n",
      "\n",
      "         [[ 0.1113]],\n",
      "\n",
      "         [[-0.0173]],\n",
      "\n",
      "         [[-0.1274]],\n",
      "\n",
      "         [[-0.0447]],\n",
      "\n",
      "         [[-0.2639]],\n",
      "\n",
      "         [[-0.1097]],\n",
      "\n",
      "         [[ 0.1017]],\n",
      "\n",
      "         [[ 0.0018]],\n",
      "\n",
      "         [[-0.0780]],\n",
      "\n",
      "         [[-0.1222]],\n",
      "\n",
      "         [[ 0.3291]],\n",
      "\n",
      "         [[ 0.0326]],\n",
      "\n",
      "         [[ 0.0404]],\n",
      "\n",
      "         [[ 0.0067]],\n",
      "\n",
      "         [[ 0.1169]],\n",
      "\n",
      "         [[-0.0489]],\n",
      "\n",
      "         [[ 0.0808]],\n",
      "\n",
      "         [[-0.0101]],\n",
      "\n",
      "         [[ 0.0546]],\n",
      "\n",
      "         [[ 0.0948]],\n",
      "\n",
      "         [[-0.0380]],\n",
      "\n",
      "         [[-0.3870]],\n",
      "\n",
      "         [[-0.0542]],\n",
      "\n",
      "         [[ 0.0464]],\n",
      "\n",
      "         [[-0.0046]],\n",
      "\n",
      "         [[-0.0066]],\n",
      "\n",
      "         [[ 0.2135]],\n",
      "\n",
      "         [[-0.0026]],\n",
      "\n",
      "         [[-0.0286]],\n",
      "\n",
      "         [[ 0.0419]],\n",
      "\n",
      "         [[ 0.0978]],\n",
      "\n",
      "         [[ 0.0609]],\n",
      "\n",
      "         [[ 0.0753]],\n",
      "\n",
      "         [[-0.1545]],\n",
      "\n",
      "         [[ 0.0083]],\n",
      "\n",
      "         [[ 0.0489]],\n",
      "\n",
      "         [[ 0.2948]],\n",
      "\n",
      "         [[-0.0084]],\n",
      "\n",
      "         [[ 0.2160]],\n",
      "\n",
      "         [[-0.0285]]]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([   nan, 0.1179], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in list(net.parameters()):\n",
    "    if p.isnan().any():\n",
    "        print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = net(torch.from_numpy(inputs[0])[None, ...].to('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "       device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o[1:3, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "         ...,\n",
       "         [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf,  ..., -inf, -inf, -inf]]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o[3:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_out = net.backbone(torch.from_numpy(inputs[0])[None, ...].to('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbone_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_out = net.backbone.downscale(torch.from_numpy(inputs[0])[None, ...].to('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(net.backbone.downscale.md.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(net.backbone.downscale.md.parameters())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(net.backbone.upscale.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.from_numpy(inputs).to('cuda:0')\n",
    "print(y)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for layer in list(net.backbone.downscale.md.children()):\n",
    "        x = layer(y)\n",
    "\n",
    "        if x.isnan().any() or x.isinf().any():\n",
    "            print(layer, x.shape, y.shape)\n",
    "            break\n",
    "\n",
    "        y = x.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, 'centernet.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('centernet.pth')\n",
    "model.to(torch.device('cpu'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id = 101243\n",
    "img_meta = ct.loadImgs(ids=[img_id])[0]\n",
    "plt.imshow(plt.imread(images_path / img_meta['file_name']))\n",
    "annIds = ct.getAnnIds(imgIds=img_meta['id'])\n",
    "anns = ct.loadAnns(annIds)\n",
    "ct.showAnns(anns)\n",
    "plt.title(f\"GT: {img_meta['id']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, anno = ds_val[ds_val.ids.index(str(img_id))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(img[None, :])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_preds = []\n",
    "img_ids = []\n",
    "pto = PointsToObjects()\n",
    "\n",
    "for num, img_id in enumerate(tqdm(ds_val.ids)):\n",
    "    img_id = int(img_id)\n",
    "    img_meta = ct.loadImgs(ids=[img_id])[0]\n",
    "    with torch.no_grad():\n",
    "        x = ds_val[num][0]\n",
    "        pred = model(\n",
    "            x[None, ...]\n",
    "        )[0]\n",
    "        prepared_preds.append(\n",
    "            pto(pred[None, ...])\n",
    "        )\n",
    "        img_ids.append(img_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prpr = []\n",
    "for i in prepared_preds:\n",
    "    unf = {}\n",
    "    res = i[i[:, :, -1] > 0.2].detach()\n",
    "    boxes = res[:, :4].clone()\n",
    "    b = boxes.clone()\n",
    "    boxes[:, 2] = (b[:, 2] - b[:, 0])\n",
    "    boxes[:, 3] = (b[:, 3] - b[:, 1])\n",
    "    unf['boxes'] = boxes.numpy()\n",
    "    unf['scores'] = res[:, -1].detach().numpy()\n",
    "    #print(unf['boxes'].shape[0], unf['scores'].shape[0])\n",
    "    prpr.append(unf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.concatenate([u['scores'] for u in prpr], axis=0)\n",
    "boxes = np.concatenate([u['boxes'] for u in prpr], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = []\n",
    "for num, i in enumerate(img_ids):\n",
    "    image_ids += [i] * len(prpr[num]['boxes'])\n",
    "image_ids = np.array(image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(image_ids.tolist()),\n",
    "    len(boxes[:, 0].tolist()),\n",
    "    len(boxes[:, 1].tolist()),\n",
    "    len(boxes[:, 2].tolist()),\n",
    "    len(boxes[:, 3].tolist()),\n",
    "    len(scores.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_detections_to_cocotext_json(\n",
    "    image_ids = image_ids.tolist(),\n",
    "    xlefts=boxes[:, 0].tolist(),\n",
    "    ytops=boxes[:, 1].tolist(),\n",
    "    widths=boxes[:, 2].tolist(),\n",
    "    heights=boxes[:, 3].tolist(),\n",
    "    scores=scores.tolist(),\n",
    "    path='predictions.json'\n",
    ");\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AP calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap, prec, rec = evaluate_ap_from_cocotext_json(\n",
    "    coco_text=ct,\n",
    "    path='predictions.json'\n",
    ")\n",
    "print(f\"  AP  val: {ap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(prec, rec)\n",
    "plt.xlabel('precision')\n",
    "plt.ylabel('recall')\n",
    "plt.title('PR curve')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9de35f58b490df70c64b1db97b3e54dc2a525ab2d36632bb7ddc91e7dd00330b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
