{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avashchilko/abbyy9sem/course_cvdl/.venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from torchvision.datasets import CocoDetection\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abbyy_course_cvdl_t3.coco_text import COCO_Text\n",
    "from abbyy_course_cvdl_t3 import coco_evaluation\n",
    "from abbyy_course_cvdl_t3.utils import evaluate_ap_from_cocotext_json\n",
    "from abbyy_course_cvdl_t3.utils import dump_detections_to_cocotext_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/avashchilko/abbyy9sem/course_cvdl/task2\n"
     ]
    }
   ],
   "source": [
    "cd ../task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abbyy_course_cvdl_t2\n",
    "from abbyy_course_cvdl_t2.convert import PointsToObjects, ObjectsToPoints\n",
    "from abbyy_course_cvdl_t2.impl.train import train\n",
    "from abbyy_course_cvdl_t2.impl.data import CocoTextDetection, CocoDetectionPrepareTransform\n",
    "from abbyy_course_cvdl_t2.network import CenterNet\n",
    "from abbyy_course_cvdl_t2.loss import CenterNetLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/avashchilko/abbyy9sem/course_cvdl/task3\n"
     ]
    }
   ],
   "source": [
    "cd ../task3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = Path(\"/home/avashchilko/coco2014\")\n",
    "anno_path = base / 'cocotext.v2.json'\n",
    "images_path = base / 'train'\n",
    "\n",
    "assert anno_path.exists(), \"Set your own path to annotation\"\n",
    "assert images_path.exists(), \"Set your own path to images\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:01.877434\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "ct = COCO_Text(anno_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(image_np, yc_t, xc_t, hy_t, wx_t, color=(0, 255, 0), thick=1):\n",
    "    img = np.array(image_np)\n",
    "    num_boxes = len(yc_t)\n",
    "    val = np.array(color)\n",
    "    for idx in range(num_boxes):\n",
    "        yc = yc_t[idx]\n",
    "        xc = xc_t[idx]\n",
    "        hy = hy_t[idx]\n",
    "        wx = wx_t[idx]\n",
    "        img[\n",
    "                yc - hy//2 - thick : yc - hy//2 + thick, \n",
    "                xc - wx//2 : xc + wx//2, \n",
    "        ] = val\n",
    "        img[\n",
    "                yc + hy//2 - thick : yc + hy//2 + thick, \n",
    "                xc - wx//2 : xc + wx//2, \n",
    "        ] = val\n",
    "\n",
    "        img[\n",
    "                yc - hy//2 : yc + hy//2, \n",
    "                (xc - wx//2 - thick): (xc - wx//2 + thick), \n",
    "        ] = val\n",
    "        img[\n",
    "                yc - hy//2 : yc + hy//2, \n",
    "                xc + wx//2 - thick: xc + wx//2 + thick, \n",
    "        ] = val\n",
    "    return img\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 2\n",
    "input_shape = (640, 640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = CocoTextDetection(\n",
    "    images_path,\n",
    "    anno_path,\n",
    "    transforms=CocoDetectionPrepareTransform(size=input_shape, #ids_map = {1: 0, 0: 0}\n",
    "),\n",
    "    area_fraction_threshold=1/32/32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_val = CocoTextDetection(\n",
    "    images_path,\n",
    "    anno_path,\n",
    "    transforms=CocoDetectionPrepareTransform(size=input_shape, #ids_map = {1: 0, 0: 0}\n",
    "    ),\n",
    "    area_fraction_threshold=1/32/32,\n",
    "    split='val'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = ds_train[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[334.7333, 151.4000,  31.3333,  33.6000,   1.0000,   1.0000],\n",
       "        [328.8000, 391.9500,  28.8000,  49.7000,   1.0000,   1.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(\n",
    "    ds_train, batch_size=2, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "valloader = torch.utils.data.DataLoader(\n",
    "    ds_val, batch_size=1, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avashchilko/abbyy9sem/course_cvdl/.venv/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/avashchilko/abbyy9sem/course_cvdl/.venv/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "net = CenterNet(pretrained=\"resnet34\", head_kwargs={'c_classes': n_classes}, nms_kwargs={'kernel_size': 5})\n",
    "crit = CenterNetLoss(obj_to_points=ObjectsToPoints(num_classes=n_classes, hw=160))#, l_size_lambda=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0|0; total loss:149.90386962890625\n",
      "last losses: [4495.5400390625, 0.263408899307251, 1.3125910758972168]\n"
     ]
    }
   ],
   "source": [
    "net = train(ds_train, net=net, criterion=crit, batch_size=64, epochs=100, device=torch.device(device), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, 'centernet_resnet34_640.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.load('centernet_resnet34_640.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.to(device)\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = ds_train[6]\n",
    "\n",
    "with torch.no_grad():\n",
    "    yp_heat = net(x[None].to(device)).cpu()    \n",
    "    yp = net(x[None].to(device), return_objects=True).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('class 0 heatmap')\n",
    "plt.matshow(yp_heat[0][0])\n",
    "plt.show()\n",
    "\n",
    "print('class 1 heatmap')\n",
    "plt.matshow(yp_heat[0][1])\n",
    "plt.show()\n",
    "\n",
    "print('w_heatmap')\n",
    "plt.matshow(yp_heat[0][-1])\n",
    "plt.show()\n",
    "\n",
    "print('h heatmap')\n",
    "plt.matshow(yp_heat[0][-2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(image_np, yc_t, xc_t, hy_t, wx_t, color=(0, 255, 0), thick=1):\n",
    "    img = np.array(image_np)\n",
    "    num_boxes = len(yc_t)\n",
    "    val = np.array(color)\n",
    "    for idx in range(num_boxes):\n",
    "        yc = yc_t[idx]\n",
    "        xc = xc_t[idx]\n",
    "        hy = hy_t[idx]\n",
    "        wx = wx_t[idx]\n",
    "\n",
    "        hyd2 = torch.div(hy, 2, rounding_mode='trunc')\n",
    "        wxd2 = torch.div(wx, 2, rounding_mode='trunc')\n",
    "\n",
    "        img[\n",
    "                yc - hyd2 - thick : yc - hyd2 + thick, \n",
    "                xc - wxd2 : xc + wxd2, \n",
    "        ] = val\n",
    "        img[\n",
    "                yc + hyd2 - thick : yc + hyd2 + thick, \n",
    "                xc - wxd2 : xc + wxd2, \n",
    "        ] = val\n",
    "\n",
    "        img[\n",
    "                yc - hyd2 : yc + hyd2, \n",
    "                (xc - wxd2 - thick): (xc - wxd2 + thick), \n",
    "        ] = val\n",
    "        img[\n",
    "                yc - hyd2 : yc + hyd2, \n",
    "                xc + wxd2 - thick: xc + wxd2 + thick, \n",
    "        ] = val\n",
    "    return img\n",
    "\n",
    "\n",
    "rnd = lambda x: torch.round(x).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(draw_boxes(\n",
    "    x.permute(1, 2, 0).numpy(), rnd(yp[0, :3, 0]), rnd(yp[0, :3, 1]), rnd(yp[0, :3, 2]), rnd(yp[0, :3, 3]) \n",
    "))\n",
    "plt.title('Predicted:')\n",
    "plt.show()\n",
    "plt.imshow(draw_boxes(\n",
    "    x.permute(1, 2, 0).numpy(), rnd(y[:, 0]), rnd(y[:, 1]), rnd(y[:, 2]), rnd(y[:, 3]) \n",
    "))\n",
    "plt.title('Ground truth:')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yp[0][:2], y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net = torch.load('centernet_resnet34_640.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(data, target_shape, input_shape):\n",
    "    unf = {}\n",
    "    res = data[data[:, -1] != 0]\n",
    "    boxes = res[:, :4].copy()\n",
    "    b = boxes.copy()\n",
    "    result_boxes = np.zeros_like(boxes)\n",
    "    result_boxes[:, 0] = (boxes[:, 1] - b[:, 3] / 2) * (target_shape[1] / input_shape[1])\n",
    "    result_boxes[:, 1] = (boxes[:, 0] - b[:, 2] / 2) * (target_shape[0] / input_shape[0])\n",
    "    result_boxes[:, 2] = boxes[:, 3] * (target_shape[1] / input_shape[1])\n",
    "    result_boxes[:, 3] = boxes[:, 2] * (target_shape[0] / input_shape[0])\n",
    "    unf['boxes'] = result_boxes\n",
    "    unf['scores'] = res[:, -1]\n",
    "    return unf\n",
    "\n",
    "\n",
    "def extract_results():\n",
    "    prepared_preds = []\n",
    "    img_ids = []\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    for num, img_id in enumerate(tqdm(ds_val.ids)):\n",
    "        img_id = int(img_id)\n",
    "        img_meta = ct.loadImgs(ids=[img_id])[0]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = ds_val[num][0]\n",
    "            \n",
    "            pred = net(\n",
    "                x[None, ...].to(device),\n",
    "                return_objects=True\n",
    "            )[0].detach().cpu().numpy()\n",
    "            prepared_preds.append(\n",
    "                postprocess(pred, target_shape=(img_meta['height'], img_meta['width']), input_shape=input_shape)\n",
    "            )\n",
    "            img_ids.append(img_id)\n",
    "\n",
    "    scores = np.concatenate([u['scores'] for u in prepared_preds], axis=0)\n",
    "    boxes = np.concatenate([u['boxes'] for u in prepared_preds], axis=0)\n",
    "    \n",
    "    image_ids = []\n",
    "    \n",
    "    for num, i in enumerate(img_ids):\n",
    "        image_ids += [i] * len(prepared_preds[num]['boxes'])\n",
    "    image_ids = np.array(image_ids)\n",
    "\n",
    "    dump_detections_to_cocotext_json(\n",
    "        image_ids = image_ids.tolist(),\n",
    "        xlefts=boxes[:, 0].tolist(),\n",
    "        ytops=boxes[:, 1].tolist(),\n",
    "        widths=boxes[:, 2].tolist(),\n",
    "        heights=boxes[:, 3].tolist(),\n",
    "        scores=scores.tolist(),\n",
    "        path=f'predictions.json'\n",
    "    )\n",
    "    \n",
    "    ap, prec, rec = evaluate_ap_from_cocotext_json(\n",
    "    coco_text=ct,\n",
    "    path=f'predictions.json'\n",
    "    )\n",
    "    return (ap, prec, rec, len(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best AP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap, prec, rec, n_obj = extract_results()\n",
    "\n",
    "print(f\"Итоговый скор AP на val: {ap}\")\n",
    "\n",
    "plt.plot(prec, rec)\n",
    "plt.xlabel('precision')\n",
    "plt.ylabel('recall')\n",
    "plt.title('PR curve')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4f782c782e2a32bb2802e651bce4745c161372ca597e1c0a908f1fe5f8326707"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
